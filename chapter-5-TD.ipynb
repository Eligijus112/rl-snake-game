{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal difference (TD) learning\n",
    "\n",
    "Temporal difference learning is a model-free reinforcement learning algorithm. \n",
    "\n",
    "A `model` in RL is the policy matrix: the probabilities of transitioning from one state to another. \n",
    "\n",
    "An agent in the TD learning framework does not have a model of the environment. Instead, it learns the policy matrix by interacting with the environment.\n",
    "\n",
    "# Definition recap \n",
    "\n",
    "Before diving deeper into TD learning, we need to define some additional terms.\n",
    "\n",
    "`Control` - the process of learning the optimal policy (finding the policy matrix $P$).\n",
    "\n",
    "`Prediction` - the process of learning the value $v_{\\pi} (s)$ and $q_{\\pi}(a, s)$ function with a fixed policy $\\pi$ for all $s$ and $a$ (finding the value matrix $V$).\n",
    "\n",
    "`Bootstrapping` - the process of using the current estimate of the value function to estimate the value function for the next state.\n",
    "\n",
    "`Episode` - the process of interacting with the environment until the episode is terminated (the terminal state is reached).\n",
    "\n",
    "`Step` - a single interaction with the environment. \n",
    "\n",
    "`State` - the current state of the environment.\n",
    "\n",
    "`Action` - the action taken by the agent.\n",
    "\n",
    "`Reward` - the reward received by the agent.\n",
    "\n",
    "# TD(0) algorithm \n",
    "\n",
    "The TD(0) algorithm is a model-free reinforcement learning algorithm. It is a special case of the TD learning algorithm. The basis of it is that the agent chooses an action based on policy $\\pi$, observes the reward and the next state, and then updates the value function $v_{\\pi}(s)$ for the current state $s$. \n",
    "\n",
    "The full algorithm is as follows:\n",
    "\n",
    "1. Initialize the value function $v_{\\pi}(s)$ for all $s$ arbitrarily, except for the terminal state $v_{\\pi}(s_{terminal}) = 0$. Define the number of episodes $N$.\n",
    "\n",
    "2. For each episode n = 1, 2, ..., $N$:\n",
    "\n",
    "    1. Initialize the state $s$.\n",
    "    \n",
    "    2. While the state $s$ is not terminal:\n",
    "    \n",
    "        1. Choose an action $a$ from the state $s$ using policy $\\pi$.\n",
    "        \n",
    "        2. Observe the reward $r$ and the next state $s'$.\n",
    "        \n",
    "        3. Update the value function $v_{\\pi}(s)$:\n",
    "        \n",
    "            $$v_{\\pi}(s) \\leftarrow v_{\\pi}(s) + \\alpha \\left[r + \\gamma v_{\\pi}(s') - v_{\\pi}(s)\\right]$$\n",
    "            \n",
    "        4. Set $s \\leftarrow s'$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD algorithm above works particulary well if our agent, at the end of each episode, returns to the initial state and continues the episode from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the frozen lake environment\n",
    "\n",
    "We will create an environment where our agent needs go from the initial state to the goal without falling into a random generated set of holes. \n",
    "\n",
    "Each step is rewarded with a reward of -1 and the goal state has a reward of 10. If an agent falls into the hole, it receives a reward of -10 and the episode is terminated. If the agent reaches the goal, the episode is also terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python packages \n",
    "import numpy as np\n",
    "\n",
    "def init_env(\n",
    "        n_rows: int, \n",
    "        n_cols: int,\n",
    "        step_reward: float = -1, \n",
    "        goal_reward: float = 10,\n",
    "        hole_reward: float = -10,\n",
    "        n_holes: int = 1,\n",
    "        ) -> np.array: \n",
    "    \"\"\"\n",
    "    Functionat that returns the initial environment: \n",
    "        S - the state matrix indexed by [row, col]\n",
    "        V - the initial value matrix indexed by [row, col]\n",
    "        R - the reward matrix indexed by [row, col]\n",
    "        A - the action matrix indexed by [row, col]\n",
    "        P - the probability dictionary where for each state, the keys are the actions and the values are the probabilities of the next state\n",
    "    \"\"\"\n",
    "    # Initiating the S matrix \n",
    "    S = np.arange(0, n_rows * n_cols).reshape(n_rows, n_cols)\n",
    "\n",
    "    # Creating the initial V matrix\n",
    "    V = np.zeros((n_rows, n_cols))\n",
    "\n",
    "    # The start state will be always the top left corner \n",
    "    # The goal state will be always the bottom right corner\n",
    "    # We will generate a random holes that our agent can fall in\n",
    "    # Any other state that is not the hole or the goal state will receive a step reward \n",
    "    R = np.zeros((n_rows, n_cols))\n",
    "    R.fill(step_reward)\n",
    "    R[0, 0] = step_reward\n",
    "    R[-1, -1] = goal_reward\n",
    "\n",
    "    hole_coords = []\n",
    "    for _ in range(n_holes):\n",
    "        hole_row = np.random.randint(1, n_rows - 1)\n",
    "        hole_col = np.random.randint(1, n_cols - 1)\n",
    "        R[hole_row, hole_col] = hole_reward\n",
    "\n",
    "        # Appending to the hole coordinates list\n",
    "        hole_coords.append((hole_row, hole_col))\n",
    "\n",
    "    return S, V, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14]]),\n",
       " array([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]),\n",
       " array([[ -1.,  -1.,  -1.,  -1.,  -1.],\n",
       "        [ -1.,  -1., -10.,  -1.,  -1.],\n",
       "        [ -1.,  -1.,  -1.,  -1.,  10.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_env(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
