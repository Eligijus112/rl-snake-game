{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal difference (TD) learning\n",
    "\n",
    "Temporal difference learning is a model-free reinforcement learning algorithm. \n",
    "\n",
    "A `model` in RL is the policy matrix: the probabilities of transitioning from one state to another. \n",
    "\n",
    "An agent in the TD learning framework does not have a model of the environment. Instead, it learns the policy matrix by interacting with the environment.\n",
    "\n",
    "# Definition recap \n",
    "\n",
    "Before diving deeper into TD learning, we need to define some additional terms.\n",
    "\n",
    "`Control` - the process of learning the optimal policy (finding the policy matrix $P$).\n",
    "\n",
    "`Prediction` - the process of learning the value $v_{\\pi} (s)$ and $q_{\\pi}(a, s)$ function with a fixed policy $\\pi$ for all $s$ and $a$ (finding the value matrix $V$).\n",
    "\n",
    "`Bootstrapping` - the process of using the current estimate of the value function to estimate the value function for the next state.\n",
    "\n",
    "`Episode` - the process of interacting with the environment until the episode is terminated (the terminal state is reached).\n",
    "\n",
    "`Step` - a single interaction with the environment. \n",
    "\n",
    "`State` - the current state of the environment.\n",
    "\n",
    "`Action` - the action taken by the agent.\n",
    "\n",
    "`Reward` - the reward received by the agent.\n",
    "\n",
    "`State value` - the total expected accumlated reward if the agent moves from the current state: \n",
    "\n",
    "$v_{\\pi}(s) = V(s) = \\mathbb{E} \\left[R_{t +1} + \\gamma R_{t + 2} + ... + \\gamma ^{K} R_{t+K} \\right | s_{t} = s]$\n",
    "\n",
    "# TD(0) algorithm \n",
    "\n",
    "The TD(0) algorithm is a model-free reinforcement learning algorithm. It is a special case of the TD learning algorithm. The basis of it is that the agent chooses an action based on policy $\\pi$, observes the reward and the next state, and then updates the value function $v_{\\pi}(s)$ for the current state $s$. \n",
    "\n",
    "The full algorithm is as follows:\n",
    " \n",
    "1. Initialization:\n",
    "\n",
    "1.1 Initialize the value function $v_{\\pi}(s)$ for all $s$ arbitrarily, except for the terminal state $v_{\\pi}(s_{terminal}) = 0$. \n",
    "\n",
    "1.1. Define the number of episodes $N$.\n",
    "\n",
    "1.2. Define the learning rate $\\alpha \\in (0, 1]$.\n",
    "\n",
    "1.3. Define the discount factor $\\gamma \\in [0, 1]$.\n",
    "\n",
    "2. For each episode n = 1, 2, ..., $N$:\n",
    "\n",
    "    1. Initialize the state $s$.\n",
    "    \n",
    "    2. While the state $s$ is not terminal:\n",
    "    \n",
    "        2.1. Choose an action $a$ from the state $s$ using policy $\\pi$.\n",
    "        \n",
    "        2.2. Observe the reward $r$ and the next state $s'$.\n",
    "        \n",
    "        2.3. Update the state value: $v_{\\pi}(s) \\leftarrow v_{\\pi}(s) + \\alpha \\left[r + \\gamma v_{\\pi}(s') - v_{\\pi}(s)\\right]$\n",
    "            \n",
    "        2.4. Set $s \\leftarrow s'$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD algorithm above works particulary well if our agent, at the end of each episode, returns to the initial state and continues the episode from there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the frozen lake environment\n",
    "\n",
    "We will create an environment where our agent needs go from the initial state to the goal without falling into a random generated set of holes. \n",
    "\n",
    "Each step is rewarded with a reward of -1 and the goal state has a reward of 10. If an agent falls into the hole, it receives a reward of -10 and the episode is terminated. If the agent reaches the goal, the episode is also terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing python packages \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns \n",
    "from typing import Tuple\n",
    "import time \n",
    "\n",
    "# Rectangle\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Iteration tracking \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_index_to_matplot_coords(i: int, j: int, n_cols: int) -> Tuple[int, int]:\n",
    "    \"\"\"Converts an array index to a matplot coordinate\"\"\"\n",
    "    x = j\n",
    "    y = n_cols - i - 1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(\n",
    "    M: np.array, \n",
    "    goal_coords: list = [],\n",
    "    hole_coords: list = [],\n",
    "    img_width: int = 5, \n",
    "    img_height: int = 5, \n",
    "    title: str = None,\n",
    "    ) -> None: \n",
    "    \"\"\"\n",
    "    Plots a matrix as an image.\n",
    "    \"\"\"\n",
    "    height, width = M.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    \n",
    "    for x in range(height):\n",
    "        for y in range(width):\n",
    "            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n",
    "            # so we need to invert the y coordinate to plot the matrix correctly\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "            # If there is a tuple of (x, y) in the hole_coords list, we color the cell salmon\n",
    "            elif (x, y) in hole_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='salmon'))\n",
    "            \n",
    "            ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "\n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy_matrix(P: dict, S:np.array, terminal_coords: list = [], img_width: int = 5, img_height: int = 5, title: str = None) -> None: \n",
    "    \"\"\" \n",
    "    Plots the policy matrix out of the dictionary provided; The dictionary values are used to draw the arrows \n",
    "    \"\"\"\n",
    "    height, width = S.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    for x in range(height):\n",
    "        for y in range(width):\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in terminal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "            \n",
    "            else:\n",
    "                try:\n",
    "                    # Adding the arrows to the plot\n",
    "                    if 'up' in P[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n",
    "                    if 'down' in P[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n",
    "                    if 'left' in P[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                    if 'right' in P[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print(f\"Current x and y: {x}, {y}\")\n",
    "                    \n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy(S: np.array, weight_dict: dict = {'right': 1}) -> dict:\n",
    "    # Saving all the unique states to a vector \n",
    "    states = np.unique(S)\n",
    "\n",
    "    # Getting the number of rows and columns of the S matrix\n",
    "    n_row = S.shape[0]\n",
    "    n_col = S.shape[1]\n",
    "\n",
    "    # Dictionary to hold each action for a given state\n",
    "    P = {}\n",
    "    for s in states: \n",
    "        s_dict = {}\n",
    "\n",
    "        # Checking which index is the current state in the S matrix \n",
    "        s_index = np.where(S == s)\n",
    "\n",
    "        # If the state is in the top left corner, we can only move right and down\n",
    "        if s_index == (0, 0):\n",
    "            s_dict['right'] = 0.5 * weight_dict['right']\n",
    "            s_dict['down'] = 1 - s_dict['right']\n",
    "        \n",
    "        # If the state is in the top right corner, we can only move left and down\n",
    "        elif s_index == (0, n_col - 1):\n",
    "            s_dict['left'] = 0.5\n",
    "            s_dict['down'] = 0.5\n",
    "        \n",
    "        # If the state is in the bottom left corner, we can only move right and up\n",
    "        elif s_index == (n_row - 1, 0):\n",
    "            s_dict['right'] = 0.5 * weight_dict['right']\n",
    "            s_dict['up'] = 1 - s_dict['right']\n",
    "        \n",
    "        # If the state is in the bottom right corner, we can only move left and up\n",
    "        elif s_index == (n_row - 1, n_col - 1):\n",
    "            s_dict['left'] = 0.5\n",
    "            s_dict['up'] = 0.5\n",
    "        \n",
    "        # If the state is in the first row, we can only move left, right, and down\n",
    "        elif s_index[0] == 0:\n",
    "            s_dict['right'] = 0.333 * weight_dict['right']\n",
    "            s_dict['left'] = (1 - s_dict['right']) / 2\n",
    "            s_dict['down'] =  (1 - s_dict['right']) / 2\n",
    "        \n",
    "        # If the state is in the last row, we can only move left, right, and up\n",
    "        elif s_index[0] == n_row - 1:\n",
    "            s_dict['right'] = 0.333 * weight_dict['right']\n",
    "            s_dict['left'] =  (1 - s_dict['right']) / 2\n",
    "            s_dict['up'] = (1 - s_dict['right']) / 2\n",
    "        \n",
    "        # If the state is in the first column, we can only move up, down, and right\n",
    "        elif s_index[1] == 0:\n",
    "            s_dict['right'] = 0.333 * weight_dict['right']\n",
    "            s_dict['up'] = (1 - s_dict['right']) / 2\n",
    "            s_dict['down'] = (1 - s_dict['right']) / 2\n",
    "            \n",
    "        # If the state is in the last column, we can only move up, down, and left\n",
    "        elif s_index[1] == n_col - 1:\n",
    "            s_dict['up'] = 0.333\n",
    "            s_dict['down'] = 0.333\n",
    "            s_dict['left'] = 1 - s_dict['up'] - s_dict['down']\n",
    "\n",
    "        # If the state is in the middle, we can move in all directions\n",
    "        else:\n",
    "            s_dict['right'] = 0.25 * weight_dict['right']\n",
    "            s_dict['up'] = (1 - s_dict['right']) / 3\n",
    "            s_dict['down'] = (1 - s_dict['right']) / 3\n",
    "            s_dict['left'] = (1 - s_dict['right']) / 3\n",
    "            \n",
    "        # Saving the current states trasition probabilities\n",
    "        P[s] = s_dict\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_holes(nrow: int, ncol: int, start_coords: list, hole_coords: list, nholes: int = 1) -> list:\n",
    "    \"\"\"\n",
    "    Function that generates nholes in a gridworld \n",
    "\n",
    "    The holes cannot be: \n",
    "        - in the start state\n",
    "        - in the goal state\n",
    "    \"\"\"\n",
    "    # Generating the hole coordinates \n",
    "    # The hole cannot be in the start or goal state\n",
    "    hole_coords = []\n",
    "    for _ in range(nholes):\n",
    "\n",
    "        hole_row = np.random.randint(0, nrow - 1)\n",
    "        hole_col = np.random.randint(0, ncol - 1)\n",
    "\n",
    "        while (hole_row, hole_col) in start_coords or (hole_row, hole_col) in hole_coords:\n",
    "            hole_row = np.random.randint(0, nrow - 1)\n",
    "            hole_col = np.random.randint(0, ncol - 1)\n",
    "\n",
    "        # Appending to the hole coordinates list\n",
    "        hole_coords.append((hole_row, hole_col))\n",
    "\n",
    "    return hole_coords\n",
    "\n",
    "def init_env(\n",
    "        n_rows: int, \n",
    "        n_cols: int,\n",
    "        step_reward: float = -1, \n",
    "        goal_reward: float = 10,\n",
    "        hole_reward: float = -10,\n",
    "        n_holes: int = 1,\n",
    "        random_seed: int = 42, \n",
    "        policy_weights: dict = {'right': 1}\n",
    "        ) -> np.array: \n",
    "    \"\"\"\n",
    "    Functionat that returns the initial environment: \n",
    "        S - the state matrix indexed by [row, col]\n",
    "        V - the initial value matrix indexed by [row, col]\n",
    "        R - the reward matrix indexed by [row, col]\n",
    "        A - the action matrix indexed by [row, col]\n",
    "        P - the probability dictionary where for each state, the keys are the actions and the values are the probabilities of the next state\n",
    "    \"\"\"\n",
    "    # Setting the random seed\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Initiating the S matrix \n",
    "    S = np.arange(0, n_rows * n_cols).reshape(n_rows, n_cols)\n",
    "\n",
    "    # Creating the initial V matrix\n",
    "    V = np.zeros((n_rows, n_cols))\n",
    "\n",
    "    # The start state will be always the top left corner \n",
    "    # The goal state will be always the bottom right corner\n",
    "    # We will generate a random holes that our agent can fall in\n",
    "    # Any other state that is not the hole or the goal state will receive a step reward \n",
    "    goal_coord = (n_rows - 1, n_cols - 1)\n",
    "    R = np.zeros((n_rows, n_cols))\n",
    "    R.fill(step_reward)\n",
    "    R[0, 0] = step_reward\n",
    "    R[goal_coord] = goal_reward\n",
    "\n",
    "    # Generating the hole coordinates \n",
    "    # The hole cannot be in the start or goal state\n",
    "    hole_coords = generate_holes(n_rows, n_cols, [(0, 0)], [goal_coord], n_holes)\n",
    "\n",
    "    # Setting the hole reward\n",
    "    for hole_coord in hole_coords:\n",
    "        R[hole_coord] = hole_reward\n",
    "        \n",
    "    # Initiating the policy \n",
    "    P = init_policy(S, weight_dict=policy_weights)\n",
    "\n",
    "    return S, V, R, P, hole_coords, [goal_coord]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the agent exploration abilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_move(s, S, P) -> int:\n",
    "    \"\"\"\n",
    "    Given the current state, returns the coordinates of the next state based on the current policy \n",
    "    \"\"\"\n",
    "    # Getting the current state index \n",
    "    s_index = np.where(S == s)\n",
    "\n",
    "    # Getting the current state policy\n",
    "    s_policy = P[s]\n",
    "\n",
    "    # Selecting the next action based on the current policy\n",
    "    next_action = np.random.choice(list(s_policy.keys()), p=list(s_policy.values()))\n",
    "\n",
    "    # Getting the next state coordinates based on the next action\n",
    "    try:\n",
    "        if next_action == 'up':\n",
    "            next_state = S[s_index[0] - 1, s_index[1]][0]\n",
    "        elif next_action == 'down':\n",
    "            next_state = S[s_index[0] + 1, s_index[1]][0]\n",
    "        elif next_action == 'left':\n",
    "            next_state = S[s_index[0], s_index[1] - 1][0]\n",
    "        elif next_action == 'right':\n",
    "            next_state = S[s_index[0], s_index[1] + 1][0]\n",
    "    except Exception as e: \n",
    "        print(f\"Current state: {s}\")\n",
    "        print(f'Next action: {next_action}')\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "    return next_state\n",
    "\n",
    "def get_state_coords(s, S) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the state coordinates given the state index\n",
    "    \"\"\"\n",
    "    s_index = np.where(S == s)\n",
    "    return s_index[0][0], s_index[1][0]\n",
    "\n",
    "def update_value(s, s_prime, S, P, V, R, alpha: float = 0.1, gamma: float = 0.9) -> float: \n",
    "    \"\"\"\n",
    "    Updates the current value function based on the current policy\n",
    "    \"\"\"\n",
    "    # Getting the CURRENT state's nrow and ncol index\n",
    "    s_index_now = get_state_coords(s, S)\n",
    "\n",
    "    # Getting the SELECTED state's nrow and ncol index\n",
    "    s_index_prime = get_state_coords(s_prime, S)\n",
    "\n",
    "    # Getting the reward by moving to the selected state \n",
    "    move_reward = R[s_index_prime[0], s_index_prime[1]]\n",
    "\n",
    "    # Getting the current estimated value of the selected state \n",
    "    current_value = V[s_index_now[0], s_index_now[1]]\n",
    "\n",
    "    # The next value \n",
    "    prime_value = V[s_index_prime[0], s_index_prime[1]]\n",
    "\n",
    "    # Returning the TD(0) current state value \n",
    "    return current_value + alpha * (move_reward + gamma * prime_value - current_value)\n",
    "\n",
    "def episode_exploration(S, P, V, R, terminal_state_coords: list, alpha: float = 0.1, gamma: float = 0.9) -> None: \n",
    "    \"\"\"\n",
    "    Agent exploration and value updating using TD(0) equation until a terminal state is reached\n",
    "    \"\"\"\n",
    "    # The starting state is 0 \n",
    "    s = 0 \n",
    "\n",
    "    # Keeping track of the number of moves\n",
    "    n_moves = 0\n",
    "\n",
    "    # Getting the coordinates of the s \n",
    "    s_coords = get_state_coords(s, S)\n",
    "\n",
    "    while s_coords not in terminal_state_coords:\n",
    "        # Selecting the next state based on the current policy\n",
    "        s_prime = select_move(s, S, P)\n",
    "\n",
    "        # Updating the current state value \n",
    "        V[s_coords] = update_value(s, s_prime, S, P, V, R, alpha, gamma)\n",
    "\n",
    "        # Updating the current state \n",
    "        s = s_prime\n",
    "\n",
    "        # Incrementing the number of moves\n",
    "        n_moves += 1\n",
    "\n",
    "        # Getting teh new s coords\n",
    "        s_coords = get_state_coords(s, S)\n",
    "    \n",
    "    return n_moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, V, R, P, hole_coords, goal_coard = init_env(5, 7, n_holes=4, random_seed=3)\n",
    "plot_matrix(S, goal_coords=goal_coard, hole_coords=hole_coords, title='State Matrix')\n",
    "plot_matrix(R, goal_coords=goal_coard, hole_coords=hole_coords, title='Reward Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_matrix(P, S, terminal_coords=hole_coords + goal_coard, title='Policy Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(V, goal_coords=goal_coard, hole_coords=hole_coords, title='Value Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of episodes to explore \n",
    "n_episodes = 10000\n",
    "\n",
    "# We will plot the V matrix after each episode filling the same device plot to make an animation\n",
    "number_of_walks = []\n",
    "for _ in tqdm(range(n_episodes)):\n",
    "    n = episode_exploration(S, P, V, R, terminal_state_coords=hole_coords + goal_coard, alpha=0.1, gamma=0.9)\n",
    "    number_of_walks.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_V = V.sum()\n",
    "print(f'The sum of the V matrix is: {sum_V:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the distribution of the number of moves \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(number_of_walks, fill=True)\n",
    "plt.title(f'Number of moves distribution | Mean: {np.mean(number_of_walks):.2f} | Std: {np.std(number_of_walks):.2f}')\n",
    "plt.xlabel('Number of moves')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoding np nan to the hole and goal states\n",
    "for hole_coord in hole_coords:\n",
    "    V[hole_coord] = np.nan\n",
    "\n",
    "# Plotting the V matrix\n",
    "sns.heatmap(V, annot=True, fmt='.2f', cmap='Blues', cbar=False, xticklabels=False, yticklabels=False)\n",
    "\n",
    "# Adding a black border around the hole_coords\n",
    "for hole_coord in hole_coords:\n",
    "    plt.gca().add_patch(Rectangle((hole_coord[1], hole_coord[0]), 1, 1, fill=True, edgecolor='black', lw=1, facecolor='red'))\n",
    "plt.title('Value Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assiging a different policy\n",
    "S, V, R, P, hole_coords, goal_coard = init_env(5, 7, n_holes=4, random_seed=3, policy_weights={'right': 1.5}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of episodes to explore \n",
    "n_episodes = 10000\n",
    "\n",
    "# We will plot the V matrix after each episode filling the same device plot to make an animation\n",
    "number_of_walks = []\n",
    "for _ in tqdm(range(n_episodes)):\n",
    "    n = episode_exploration(S, P, V, R, terminal_state_coords=hole_coords + goal_coard, alpha=0.1, gamma=0.9)\n",
    "    number_of_walks.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_V = V.sum()\n",
    "print(f'The sum of the V matrix is: {sum_V:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the distribution of the number of moves \n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(number_of_walks, fill=True)\n",
    "plt.title(f'Number of moves distribution | Mean: {np.mean(number_of_walks):.2f} | Std: {np.std(number_of_walks):.2f}')\n",
    "plt.xlabel('Number of moves')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoding np nan to the hole and goal states\n",
    "for hole_coord in hole_coords:\n",
    "    V[hole_coord] = np.nan\n",
    "\n",
    "# Plotting the V matrix\n",
    "sns.heatmap(V, annot=True, fmt='.2f', cmap='Blues', cbar=False, xticklabels=False, yticklabels=False)\n",
    "\n",
    "# Adding a black border around the hole_coords\n",
    "for hole_coord in hole_coords:\n",
    "    plt.gca().add_patch(Rectangle((hole_coord[1], hole_coord[0]), 1, 1, fill=True, edgecolor='black', lw=1, facecolor='red'))\n",
    "plt.title('Value Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
