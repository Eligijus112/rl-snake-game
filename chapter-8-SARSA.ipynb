{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA for continues learning tasks\n",
    "\n",
    "SARSA stands for **S**tate **A**ction **R**eward **S**tate **A**ction. The way we update the action values that our agent takes is encoded in the name of the algorithm: we make an action in a state, observe the reward and then take some precomputed values from the next state that we would end up in. \n",
    "\n",
    "The tabular SARSA update is: \n",
    "\n",
    "$$Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left( R + \\gamma Q(S^{*}, A^{*}) - Q(S, A) \\right)$$\n",
    "\n",
    "The update rule works fine for specific tabular cases, but we need to augment it when we have an infinite number of states. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continues learning task - stabilizing a moon lander \n",
    "\n",
    "Let us imagine that we are tasked of developing an algorithm to stabilize a moon lander. We want that our moon landercould self-drive unto a landing pad that is on the ground. \n",
    "\n",
    "The state that our moon lander is in can be ploted on a 2d plot and is defined by the $x$ and $y$ coordinates ($x$, $y$). The coordinates of the goal state (the landing pad) are ($x^{*}$, $y^{*}$)\n",
    "\n",
    "The physics that our moon lander moves are quite simplified. It can only do 1 action per time step and those actions are: \n",
    "\n",
    "$$A = \\{-1, 0, 1\\}$$ \n",
    "\n",
    "Where: \n",
    "\n",
    "-1 means that the rockets fire to the right, moving the moon lander to the left.\n",
    "\n",
    "0 means that our agent did not fire the rockets.\n",
    "\n",
    "+1 means that the rockets fire to the left, moving the moon lander to the right. \n",
    "\n",
    "The trajectory that our agent takes during each time step is: \n",
    "\n",
    "$$x_{t + 1} = x_{t} + a_{t} * 0.1 + unif_{[-0.1, 0.1]}$$\n",
    "\n",
    "$$y_{t + 1} = y_{t} - 0.05$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moon lander always starts at a random location generated by: \n",
    "\n",
    "$$ (unif_{[-10, 10]}, 50) $$\n",
    "\n",
    "The landing pad is in the point $(0, 0)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy randomness generator\n",
    "import numpy as np \n",
    "\n",
    "class MoonLander: \n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            rocket_strength: float = 0.1,\n",
    "            wind_strenght: float = 0.1,\n",
    "            actions: list = [-1, 0, 1]\n",
    "            ):\n",
    "        self.rocket_strength = rocket_strength\n",
    "        self.wind_strength = wind_strenght\n",
    "\n",
    "        # Initiating the lander \n",
    "        self.init_lander()\n",
    "\n",
    "    def init_lander(self, starting_y: int = 50): \n",
    "        starting_x = np.random.uniform(-10, 10) \n",
    "        self.starting_point = (starting_x, starting_y)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
