{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision processes\n",
    "\n",
    "In the previous chapter with bandits, every time we made an action the environment did not change. In this chapter we will consider environments where the state of the world changes after we make an action. We will call these environments Markov decision processes (MDPs). We will also consider the problem of finding the best policy for an MDP, which is called the policy optimization problem.\n",
    "\n",
    "![](media/chapter-3/MDP-schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the schema of a genral RL process: \n",
    "\n",
    "* An agent makes and action \n",
    "\n",
    "* The action perturbs the environment and the environment returns a reward and a new state\n",
    "\n",
    "* The agent uses the reward and the new state to update its policy\n",
    "\n",
    "* The cycle continues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the k - bandit examples, in MDP scenarios, each taken action alters the state of the environment that the agent operates in. The sequance of any MDP is: \n",
    "\n",
    "$$S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an MDP, we can define three sets: \n",
    "\n",
    "$\\mathbb{S}$ - the set of all possible states \n",
    "\n",
    "$\\mathbb{A}$ - the set of all possible actions\n",
    "\n",
    "$\\mathbb{R}$ - the set of all possible rewards\n",
    "\n",
    "Then, the dynamics of an MDP process can be defined as a probability: \n",
    "\n",
    "$p(s^{*}, r| s, a) = P(S_{t} = s^{*}, R_{t} = r | S_{t-1} = s, A_{t-1}=a)$\n",
    "\n",
    "$S_{t} \\in \\mathbb{S}, R_{t} \\in \\mathbb{R}, A_{t} \\in \\mathbb{A}$  $\\forall t$\n",
    "\n",
    "Because we are dealing with probabilities, then: \n",
    "\n",
    "$$ \\sum_{s^{*} \\in \\mathbb{S}} \\sum_{r \\in \\mathbb{R}} p(s^{*}, r| s, a) = 1 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full MDP example - vacuum robot \n",
    "\n",
    "One way to visualize and MDP is to use `transition graphs`. A transition graph is a graph where the nodes are states and the edges are actions. The edges are labeled with the probability of transitioning to a new state and the reward received. \n",
    "\n",
    "Let us assume that our robot has two states - low and high battery. The robot can recharge, wait or vacuum. The robot can only recharge when it is in the low battery state. The robot can only vacuum when it is in the high battery state. The total reward is the amount of vacuumed dust measured in grams. Alternitavely, the robot can always wait.\n",
    "\n",
    "$$ \\mathbb{S} = \\{low, high\\} $$\n",
    "\n",
    "The full action space is:\n",
    "\n",
    "$$ \\mathbb{A} = \\{recharge, wait, vacuum\\} $$\n",
    "\n",
    "![](media/chapter-3/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities seen in the graph get estimated using many trial runs. \n",
    "\n",
    "In our example, a trial ends when the robot runs out of battery. The reward is the amount of dust collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rl-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
