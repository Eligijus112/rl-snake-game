{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One armed bandits \n",
    "\n",
    "The one armed bandit is a slang for the slot machines in casinos.\n",
    "\n",
    "![](media/chapter-2/one-armed-bandit.jpg)\n",
    "\n",
    "The action here is the pulling of the lewer. Each action costs a certain amount of money. The reward is unknown. If we have a fixed budget of money and we have multiple slot machines, we may want to have a strategy as to what machine to pull. If we spend all or our money on one slot machine, we can never know what is the average return of another. \n",
    "\n",
    "# K - armed bandits \n",
    "\n",
    "![](media/chapter-2/k-armed-bandits.png)\n",
    "\n",
    "In practise, a common problem RL tries to solve is the so called k - armed bandit problem. The $K$ in the name refers to all the possible actions that an agent can take at a given time step $t$. As in the picture above, the agent (in our case, an octopus) can choose to pull one of the $K$ levers. \n",
    "\n",
    "The simplest formulation of the k-armed problem is `to find which bandit gives the highest reward without any other piece of information`. \n",
    "\n",
    "A more standart formulation is to `maximize the expected total reward over some time period, for example,\n",
    "over 1000 action selections, or time steps`.\n",
    "\n",
    "# K - armed bandit simulation \n",
    "\n",
    "Let us assume that we know the distribution of returns for each bandit. We can simulate the returns of each bandit by sampling from the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the randomnes generator \n",
    "import numpy as np\n",
    "\n",
    "# Importing the plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# Dataframe creation \n",
    "import pandas as pd\n",
    "\n",
    "# Iteration tracking \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 5 bandits each with different normal distributions \n",
    "means = [5, 6.5, 7, 8.5, 9.5]\n",
    "\n",
    "# Generating the returns for each bandit\n",
    "bandits = [np.random.normal(m, 1, 1000) for m in means]\n",
    "\n",
    "# Creating a dataframe for the returns\n",
    "df = pd.DataFrame(bandits).T \n",
    "\n",
    "# Melting the dataframe\n",
    "df = pd.melt(df)\n",
    "df.columns=['bandit', 'return']\n",
    "\n",
    "# Ploting the returns for each bandit\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.violinplot(x='bandit', y='return', data=df, ax=ax)\n",
    "\n",
    "# Adding the labels of  the mean returns\n",
    "for i, m in enumerate(means):\n",
    "    # The background color of the text is set to white\n",
    "    ax.text(i, m, f'{m:.2f}', ha='center', va='center', color='white', fontsize=15, fontweight='bold')\n",
    "\n",
    "ax.set_title('Bandit returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, the creators of the bandits, know that the best strategy is to always play on the 5th bandit, because the mean reward after each pull is the highest. The problem is that an agent does not know this. It has to learn by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explotation vs exploration \n",
    "\n",
    "One of the fundamental problems in reinforced learning is the explotation vs exploration part.  \n",
    "\n",
    "**Exploration** allows an agent to improve its current knowledge about each action, hopefully leading to long-term benefit. Improving the accuracy of the estimated action-values, enables an agent to make more informed decisions in the future.\n",
    "\n",
    "**Exploitation** on the other hand, chooses the greedy action to get the most reward by exploiting the agentâ€™s current action-value estimates. But by being greedy with respect to action-value estimates, and agent may not actually get the most reward and lead to sub-optimal behaviour. \n",
    "\n",
    "When an agent explores, it gets more accurate estimates of action-values. And when it exploits, it might get more reward. It cannot, however, choose to do both simultaneously, which is also called the `exploration-exploitation dilemma`.\n",
    "\n",
    "# Notations \n",
    "\n",
    "$\\mathbb{A} = \\{a_{1}, a_{2}, ..., a_{n}\\}$ - all the available actions our agent can take.  \n",
    "\n",
    "$A_{t}$ - action taken at time step $t$.\n",
    "\n",
    "$R_{t} \\in \\mathbb{R}$ - the reward at time step $t$. \n",
    "\n",
    "$q_{*}(a) = \\mathbb{E}\\left[R_{t} | A_{t} = a\\right]$ - the expected value of action $a$, also called the optimal action value. \n",
    "\n",
    "$Q_{t}(a)$ - the estimated value of action $a$ at time step $t$. We would like to estimate this value as close as possible to the optimal action value $q_{*}(a)$. \n",
    "\n",
    "# Exploration and exploitation in k - armed bandits \n",
    "\n",
    "## Action - value estimation \n",
    "\n",
    "In our 5 armed bandit example, the set of all possible actions is: \n",
    "\n",
    "$$ A = \\{0, 1, 2, 3, 4\\} $$\n",
    "\n",
    "The action value estimate at time step $t$ for a given action $a$ has a recursive definition:\n",
    "\n",
    "$$ Q_{t}(a) = Q_{t-1}(a) + \\alpha \\left[R_{t-1} - Q_{t-1}(a)\\right] $$\n",
    "\n",
    "$ \\forall a \\in A$\n",
    "\n",
    "$\\alpha \\in [0, 1]$\n",
    "\n",
    "The greedy action is an action that: \n",
    "\n",
    "$$ a_{greedy} = \\underset{a \\in A}{\\operatorname{argmax}} Q_{t}(a) $$\n",
    "\n",
    "## Action - value estimation in practise \n",
    "\n",
    "Let us clearify the theoretical examples with the help of a coding example. \n",
    "\n",
    "Imagine that you are an RL agent and see 5 bandit machines for the first time. Your creators want you to come up with the optimal strategy of winning. Thus, initialy, the action values are set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Q(n_bandits):\n",
    "    \"\"\"Initialize the action-value function Q with zeros\"\"\"\n",
    "    Q = np.zeros(n_bandits)\n",
    "    return Q\n",
    "\n",
    "# Setting the initial action - values for the bandits to be 0 \n",
    "Q = init_Q(len(means)) \n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, there were no actions taken and thus every action value is 0. Let us define the function that updates the given action value estimate with the new reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q:list, epsilon: float = 1.0) -> int:\n",
    "    \"\"\"\n",
    "    Selects an action using the epsilon-greedy policy.\n",
    "\n",
    "    If espilon is 1, the action is selected randomly.\n",
    "    If epsilon is 0, the action is selected greedily.\n",
    "    \"\"\"\n",
    "    if (np.random.random() < epsilon) or (np.all(Q == 0)):\n",
    "        action = np.random.choice(range(len(Q)))\n",
    "    else:\n",
    "        action = np.argmax(Q)\n",
    "    return action\n",
    "\n",
    "# Defining the reward for each action \n",
    "rewards_generator = {\n",
    "    0: lambda: np.random.normal(means[0], 1),\n",
    "    1: lambda: np.random.normal(means[1], 1),\n",
    "    2: lambda: np.random.normal(means[2], 1),\n",
    "    3: lambda: np.random.normal(means[3], 1),\n",
    "    4: lambda: np.random.normal(means[4], 1)\n",
    "}\n",
    "\n",
    "# Defining the function that gets the reward based on the action\n",
    "def get_reward(action: int) -> float:\n",
    "    return rewards_generator[action]()\n",
    "\n",
    "def update_Q(Q:list, action: int, alpha: float = 0.1) -> list:\n",
    "    \"\"\"\n",
    "    Updates the action-value function using the sample-average method.\n",
    "    \"\"\"\n",
    "    Q[action] += alpha * (get_reward(action) - Q[action])\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do one simulation and see what is the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a random action\n",
    "action = select_action(Q, epsilon = 1.0) \n",
    "\n",
    "# Getting the rewward \n",
    "reward = get_reward(action)\n",
    "\n",
    "# Updating the action-value function\n",
    "Q = update_Q(Q, action)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the robot has taken one action ant that action produced a certain reward. If we would instruct our agent to now maximize the reward, he would only make the choice that he has made. But we want him to learn and maybe other bandits are better than the one he has seen. \n",
    "\n",
    "## Pure exploration algorithm\n",
    "\n",
    "Let us simulate the case where our algorithm only chooses the bandit to use randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of actions \n",
    "n_actions = 1000\n",
    "rewards = []\n",
    "Q = init_Q(len(means))\n",
    "for action in range(n_actions):\n",
    "    # Selecting an action\n",
    "    action = select_action(Q, epsilon = 1.0) \n",
    "\n",
    "    # Getting the rewward \n",
    "    reward = get_reward(action)\n",
    "\n",
    "    # Appending to the list of rewards\n",
    "    rewards.append(reward)\n",
    "\n",
    "# Plotting the rewards\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(rewards)\n",
    "ax.set_title('Rewards')\n",
    "ax.set_xlabel('Actions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Epsilon greedy algorithm \n",
    "\n",
    "You may have seen in the `select_action()` function that there is a parameter titled `epsilon`. This parameter is also known as the `exploration rate`. For a subset of time steps, we say to our agent that instead of chosing the best action **seen so far**, he should explore and try out other actions. \n",
    "\n",
    "Thus, by default, our agent choses the argument that maximizes the action value estimate: \n",
    "\n",
    "$$\\argmax_{a} Q_{t}(a)$$ \n",
    "\n",
    "This is called the `greedy` action. The $\\epsilon$ - greedy algorithm is a simple way to balance exploration and exploitation by alowing our agent to explore the space of actions with a probability of $\\epsilon$.  \n",
    "\n",
    "In the epislon-greedy algorithm analysis, we tend to do the runs thousands of times and then average the rewards at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the lists of epsilon\n",
    "epsilons = [0.0, 0.1, 0.4, 0.8, 1.0]\n",
    "\n",
    "# Defining the number of actions per run\n",
    "n_actions = 500\n",
    "\n",
    "# Defining the number of runs \n",
    "n_runs = 2000\n",
    "\n",
    "# Master dataframe \n",
    "d = pd.DataFrame()\n",
    "Q_df = pd.DataFrame()\n",
    "\n",
    "for epsilon in tqdm(epsilons):\n",
    "    runs = []\n",
    "    Q_run = []\n",
    "    for run in range(n_runs):\n",
    "        rewards = []\n",
    "        Q = init_Q(len(means))\n",
    "        for action in range(n_actions):\n",
    "            # Selecting an action\n",
    "            action = select_action(Q, epsilon = epsilon) \n",
    "\n",
    "            # Getting the rewward \n",
    "            reward = get_reward(action)\n",
    "\n",
    "            # Updating the action-value function\n",
    "            Q = update_Q(Q, action)\n",
    "\n",
    "            # Appending to the list of rewards\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # Appending the rewards to the runs\n",
    "        runs = np.append(runs, rewards)\n",
    "\n",
    "        # Appending the action-value function to the list of action-value functions\n",
    "        Q_run.append(Q)\n",
    "    \n",
    "    # Averaging the Q_run and appending to Q_eval \n",
    "    Q_run = np.mean(Q_run, axis=0)\n",
    "\n",
    "    # Converting to dataframe \n",
    "    Q_run = pd.DataFrame(Q_run, columns=['Q'])\n",
    "    Q_run['bandit'] = Q_run.index\n",
    "    Q_run['epsilon'] = epsilon\n",
    "\n",
    "    Q_df = pd.concat([Q_df, Q_run])\n",
    "\n",
    "    # Averaging \n",
    "    runs = runs.reshape(n_runs, n_actions)\n",
    "    runs = runs.mean(axis=0)\n",
    "\n",
    "    df = pd.DataFrame(runs, columns=['reward'])\n",
    "    df['epsilon'] = epsilon\n",
    "    df['step'] = df.index\n",
    "\n",
    "    d = pd.concat([d, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the rewards\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(x='step', y='reward', hue='epsilon', data=d, ax=ax)\n",
    "ax.set_title('Average reward at each step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting a barplot of the evaluated q values\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.barplot(x='bandit', y='Q', hue='epsilon', data=Q_df, ax=ax, edgecolor='black', linewidth=1)\n",
    "ax.set_title('Average Q values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B testing using RL algorithm \n",
    "\n",
    "This is a small bonus section. The bandit problem can be applied to the very popular A/B testing problem. \n",
    "\n",
    "The schema is the following: \n",
    "\n",
    "![](media/chapter-2/a_b_test.png)\n",
    "\n",
    "A user logs on to a website where a backend robot (our `agent`) introduces randomly either green or yellow buttons to click (`actions`). After clicking these buttons, the user then proceeds to the next page where he can shop around. The final value of a shopping carts is relayed back to the agent (`reward` from the `environment`). This information is digested by the agent and the agent stores this information. \n",
    "\n",
    "At the end of the experiment run, the agent can say the average spending by customers when clicked on the either green or yellow buttons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('rl-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
