{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic overview\n",
    "\n",
    "From [wiki](https://en.wikipedia.org/wiki/Reinforcement_learning): \n",
    "\n",
    "***Reinforcement learning (RL)** is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward.*\n",
    "\n",
    "The 4 main components of any RL algorithm are therefore the following:\n",
    "\n",
    "* `Agent` - an entity (computer program) that makes decisions. \n",
    "\n",
    "* `Action` - decision made by an agent. \n",
    "\n",
    "* `Environment` - an interface for the agents to interact with. The environment accepts actions and responds with the result and a new set of observations. \n",
    "\n",
    "* `Reward` - a function that assigns a value (reward) for each action that an agent can take. \n",
    "\n",
    "The interaction between the 4 components: \n",
    "\n",
    "![](media/chapter-1/reinforcement-learning-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the agent learn? \n",
    "\n",
    "The goal of RL is to make the agent make actions that maximize the rewards. The programmer (you) must define an environment and a reward generated function such that the agent learns with each iteration and \"good\" actions are rewarded while \"bad\" actions are penalized.\n",
    "\n",
    "To put it mathematicaly, the reward function maps the set of all actions made in the environment and assigns a reward value. \n",
    "\n",
    "If we define \n",
    "\n",
    "$A$ - action set \n",
    "\n",
    "$f_{E}$ - environment function \n",
    "\n",
    "$R$ - reward set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the output that gets feeded to the agent is: \n",
    "\n",
    "$f_{E}: A \\rightarrow R$\n",
    "\n",
    "Or \n",
    "\n",
    "$f(a) = r$, $a \\in A$, $r \\in R$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basicaly, an agent keeps a ledger in his internal memory that maps each action to a certain reward. After the reward is received, the internal state is updated. If we define the internal state at time $t$ as $w_{t}$, then the high level logic of an agent \"learning\" is: \n",
    "\n",
    "$$w_{t + 1} = w_{t} + \\alpha f_{E}(a_{t}) $$\n",
    "\n",
    "Where \n",
    "\n",
    "$\\alpha$ - a positive constant; Learning rate.\n",
    "\n",
    "$a_{t}$ - action taken at time $t$. \n",
    "\n",
    "$w_{t}$ - internal state at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](media/chapter-1/rl-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each action, the agent updates the internal state and then the cycle is repeated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42188ee9869e2b119220b29afa41430f1e45d9b6ba28e06c978bb38ae44da5c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('snake_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
