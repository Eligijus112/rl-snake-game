{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array math \n",
    "import numpy as np \n",
    "\n",
    "# Iteration tracking \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Type hinting \n",
    "from typing import Tuple\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "# Os traversal \n",
    "import os "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning \n",
    "\n",
    "Q learning is an off-policy TD algorithm to approximate the best optimal policy. \n",
    "\n",
    "Definitions: \n",
    "\n",
    "*Q value* - the value given for an action $a$ taken in state $s$. To put it more mathematically, it maps the actions and states to the real number plane: \n",
    "\n",
    "$$Q(a \\in \\mathbb{A}, s \\in \\mathbb{S}) \\rightarrow q \\in  \\mathbb{R}$$\n",
    "\n",
    "In practise, the bigger the q value for an action is, the \"better\" it is for the agent to take. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q table* - a matrix storing the Q values where each row is a state and each column is an action. \n",
    "\n",
    "*$\\epsilon$-greedy policy* - the policy where, at every decision point, the agent takes a random action with the probability of 1 - $\\epsilon$ and the action which has the biggest q value with the probability of $\\epsilon$, where $\\epsilon \\in [0, 1]$.\n",
    "\n",
    "*off-policy algorithm* - algorithms where the agent updates it's policy not using it's behavior policy. \n",
    "\n",
    "*behavior policy* - the policy used by the agent to make an action.\n",
    "\n",
    "Full algorithm for Q learning: \n",
    "\n",
    "1. Hyperparameters: \n",
    "\n",
    "1.1 Define step size $\\alpha \\in (0, 1]$\n",
    "\n",
    "1.2 Define $\\epsilon \\in [0, 1]$\n",
    "\n",
    "1.3 Define the discount factor $\\gamma \\in (0, 1]$\n",
    "\n",
    "1.4 Initialize the Q table where all the values are arbitraty except for terminal states $Q(terminal, *) = 0$\n",
    "\n",
    "1.5 Define the number of episodes $N$. \n",
    "\n",
    "2. Iterate for 1 to $N$:\n",
    "\n",
    "2.1 Pick a starting state s.\n",
    "\n",
    "2.2 Iterate until the agent reaches a terminal state: \n",
    "\n",
    "2.2.1 From the given state, pick an action $A$ using epsilon-greedy policy \n",
    "\n",
    "2.2.2 Take action $A$, observe the transition state $s^{'}$ and the reward $r$\n",
    "\n",
    "**2.2.3 Update the current Q value estimate:** \n",
    "\n",
    "$$ Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left(r + \\gamma \\max_{a} Q(s^{'}, a) - Q(S, A)\\right)$$\n",
    "\n",
    "2.2.4 Set $s$ $\\leftarrow$ $s^{'}$\n",
    "\n",
    "The 2.2.3 update is the main part of the Q algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in step 2.2.1, the behavior policy is the epslion-greedy policy. \n",
    "\n",
    "But when we update the current Q value, we do not use that behaviour policy - we update the current Q value from the future state Q value and not use the epsilon-greedy policy anywhere. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze escape problem \n",
    "\n",
    "We will create a maze where: \n",
    "\n",
    "The agent starts at the bottom left corner of the maze. \n",
    "\n",
    "The goal is to reach the top right corner of the maze. \n",
    "\n",
    "In the middle of the maze we will generate some random wall blocks. \n",
    "\n",
    "When an agent tries to transition to a wall block, it will stay in the same state with a reward of -1. \n",
    "\n",
    "Every movement will give a reward of -0.1.\n",
    "\n",
    "Reaching the goal state terminates the episode and returns a reward of 1. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the ploting functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_index_to_matplot_coords(i: int, j: int, n_cols: int) -> Tuple[int, int]:\n",
    "    \"\"\"Converts an array index to a matplot coordinate\"\"\"\n",
    "    x = j\n",
    "    y = n_cols - i - 1\n",
    "    return x, y\n",
    "\n",
    "def plot_matrix(\n",
    "    M: np.array, \n",
    "    goal_coords: list = [],\n",
    "    hole_coords: list = [],\n",
    "    start_coords: list = [],\n",
    "    highlight_coords: list = [],\n",
    "    img_width: int = 5, \n",
    "    img_height: int = 5, \n",
    "    title: str = None,\n",
    "    filename: str = None,\n",
    "    ) -> None: \n",
    "    \"\"\"\n",
    "    Plots a matrix as an image.\n",
    "    \"\"\"\n",
    "    height, width = M.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    \n",
    "    for x in range(height):\n",
    "        for y in range(width):\n",
    "            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n",
    "            # so we need to invert the y coordinate to plot the matrix correctly\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='lightgreen'))\n",
    "            # If there is a tuple of (x, y) in the hole_coords list, we color the cell salmon\n",
    "            elif (x, y) in hole_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='salmon'))\n",
    "            # If there is a tuple of (x, y) in the start_coords list, we color the cell yellow\n",
    "            elif (x, y) in start_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='yellow'))\n",
    "            # If there is a tuple of (x, y) in the highlight_coords list, we color the cell lightblue\n",
    "            elif (x, y) in highlight_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='lightblue'))\n",
    "\n",
    "            ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "\n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)\n",
    "    if filename is not None: \n",
    "        plt.savefig(filename)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_state_visits(\n",
    "        S: np.array, \n",
    "        visit_dict: dict, \n",
    "        img_width: int = 5,\n",
    "        img_height: int = 5,\n",
    "        ): \n",
    "    \"\"\"\n",
    "    Plots the states and colors them by the number of visits.\n",
    "\n",
    "    The more visits a state has, the darker the color.\n",
    "    \"\"\"\n",
    "    # Get the maximum number of visits\n",
    "    max_visits = max(visit_dict.values())\n",
    "    # Get the number of rows and columns\n",
    "    n_rows, n_cols = S.shape\n",
    "    # Create a new matrix to hold the number of visits\n",
    "    V = np.zeros((n_rows, n_cols))\n",
    "    # Iterate through the visit dictionary and update the V matrix\n",
    "    for s, visits in visit_dict.items():\n",
    "        # Converting the state to an array index\n",
    "        s_index = np.where(S == s)\n",
    "        \n",
    "        row, col = s_index[0][0], s_index[1][0]\n",
    "        \n",
    "        V[row, col] = visits\n",
    "\n",
    "    # Coloring the goal, hole, and start cells\n",
    "    height, width = S.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "    # Ploting the matrix\n",
    "    sns.heatmap(V, cmap='Blues', cbar=False, annot=True, fmt='.0f', ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the maze world "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_coords(s, S) -> tuple:\n",
    "    \"\"\"\n",
    "    Returns the state coordinates given the state index\n",
    "    \"\"\"\n",
    "    s_index = np.where(S == s)\n",
    "    return s_index[0][0], s_index[1][0]\n",
    "\n",
    "def init_maze(nrow: int, ncol: int, maze_density: int, seed: int) -> Tuple:\n",
    "    \"\"\"\n",
    "    Creates an array of states in a maze environment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    maze : np.array\n",
    "        A 2D array of states in a maze environment.\n",
    "    wall_coords : list\n",
    "        A 1D array of wall states in the maze environment.\n",
    "    start_coords: tuple\n",
    "        A tuple of start coordinates in the maze environment.\n",
    "    goal_coords: tuple\n",
    "        A tuple of goal coordinates in the maze environment.\n",
    "    \"\"\"\n",
    "    # Setting the seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Initiating the empty maze\n",
    "    maze = np.arange(0, nrow * ncol).reshape(nrow, ncol)\n",
    "\n",
    "    # Creating the wall states\n",
    "    wall_states = np.random.choice(range(1, nrow * ncol), size=maze_density, replace=False)\n",
    "\n",
    "    # Getting the list of wall coordinates\n",
    "    wall_coords = [get_state_coords(s, maze) for s in wall_states]\n",
    "\n",
    "    # Generating a starting state from the states that are NOT walls\n",
    "    start_state = np.random.choice(np.setdiff1d(maze, wall_states))\n",
    "\n",
    "    # Getting the starting coordinates\n",
    "    start_coords = get_state_coords(start_state, maze)\n",
    "\n",
    "    # Generating a goal state from the states that are NOT walls and NOT the starting state\n",
    "    goal_state = np.random.choice(np.setdiff1d(np.setdiff1d(maze, wall_states), start_state))\n",
    "\n",
    "    # Getting the goal coordinates\n",
    "    goal_coords = get_state_coords(goal_state, maze)\n",
    "\n",
    "    # return maze, wall_states\n",
    "    return maze, wall_coords, [start_coords], [goal_coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze, walls, start, goal = init_maze(6, 6, 8, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(maze, goal_coords=goal, hole_coords=walls, start_coords=start, title=\"Maze\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The policy and the movement rules \n",
    "\n",
    "The policy will be uniform and the agent can move in any direction with equal probabilities. \n",
    "\n",
    "If an agent tries to move to a wall block, it will stay in the same state.\n",
    "\n",
    "If the agent tries to go out of bounds - it will stay in the same state.\n",
    "\n",
    "When the agent reaches its goal state, the episode terminates and the agent is returned to the starting state. \n",
    "\n",
    "# The agent \n",
    "\n",
    "The agent will be an object created by the class `Agent`. All the uptades of the q table will be done internally in the agent object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        nrow_maze: int,\n",
    "        ncol_maze: int,\n",
    "        actions: list = [0, 1, 2, 3],\n",
    "        rewards: dict = {\n",
    "            'step': -1, \n",
    "            'wall': -10,\n",
    "            'goal': 10,\n",
    "        }, \n",
    "        gamma: float = 0.9,\n",
    "        alpha: float = 0.1,\n",
    "        epsilon: float = 0.1,\n",
    "        seed: int = 42,\n",
    "        ) -> None:\n",
    "        \"\"\"\n",
    "        Creates an agent for the maze environment.\n",
    "        \"\"\"\n",
    "        self.nrow_maze = nrow_maze\n",
    "        self.ncol_maze = ncol_maze\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.actions = actions\n",
    "\n",
    "        # By default, the starting index is 0 0 \n",
    "        self.start_state = 0\n",
    "\n",
    "        # By default, the goal index is the last index\n",
    "        self.goal_state = nrow_maze * ncol_maze - 1\n",
    "\n",
    "        # Creating the random generator with a fixed seed\n",
    "        self.random_generator = np.random.default_rng(seed)\n",
    "    \n",
    "        # Creating the maze; We will denote it internaly as S \n",
    "        self.init_S_table()\n",
    "\n",
    "        # Initiating the Q-table \n",
    "        self.init_Q_table()\n",
    "\n",
    "        # Saving the initial past_action and past_state\n",
    "        self.past_action = None\n",
    "        self.past_state = None\n",
    "\n",
    "        # Creating the action name dictionary \n",
    "        self.action_name_dict = {\n",
    "            0: 'up',\n",
    "            1: 'down',\n",
    "            2: 'left',\n",
    "            3: 'right',\n",
    "        }\n",
    "\n",
    "        # Counter for the number of times our agent has seen the terminal state\n",
    "        self.num_goal_reached = 0\n",
    "\n",
    "        # Counter for each state and how many times the agent visited each \n",
    "        self.state_visit_counter = {}\n",
    "\n",
    "        # Empty dictionary of states visition paths\n",
    "        self.state_visit_paths = {}\n",
    "\n",
    "        # Placeholder for the current episode of learning \n",
    "        self.current_episode = 0\n",
    "\n",
    "    def increment_state_visit(self, state) -> None:\n",
    "        \"\"\"\n",
    "        Increments the state visit counter for the state.\n",
    "        \"\"\"\n",
    "        if state in self.state_visit_counter:\n",
    "            self.state_visit_counter[state] += 1\n",
    "        else:\n",
    "            self.state_visit_counter[state] = 1\n",
    "\n",
    "    def get_most_recent_action(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the name of the most recent action.\n",
    "        \"\"\"\n",
    "        return self.action_name_dict[self.past_action]\n",
    "\n",
    "    def init_S_table(self): \n",
    "        \"\"\"\n",
    "        Creates an internal S table where the rows of the table are \n",
    "        the states and the columns are the actions.\n",
    "        \"\"\"\n",
    "        self.S = np.arange(0, self.nrow_maze * self.ncol_maze).reshape(self.nrow_maze, self.ncol_maze)\n",
    "\n",
    "    def init_Q_table(self): \n",
    "        \"\"\"\n",
    "        Creates an internal q table where the rows of the table are \n",
    "        the states and the columns are the actions.\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros((self.S.size, len(self.actions)))\n",
    "\n",
    "    def init_reward_dict(self):\n",
    "        \"\"\"\n",
    "        Creates a dictionary where the keys are the states and the values are the rewards for transitioniting to that state.\n",
    "        \"\"\"\n",
    "        if self.rewards.get('step') is None:\n",
    "            raise ValueError(\"You must specify a reward for taking a step.\")\n",
    "        else:\n",
    "            self.reward_dict = {s: self.rewards['step'] for s in self.S.flatten()}\n",
    "        \n",
    "        if self.rewards.get('goal') is None: \n",
    "            raise ValueError(\"You must specify a reward for reaching the goal state.\")   \n",
    "        else:\n",
    "            self.reward_dict[self.goal_state] = self.rewards['goal']\n",
    "\n",
    "        if self.rewards.get('wall') is not None:\n",
    "            # Setting the reward for the wall states\n",
    "            for wall_state in self.wall_states:\n",
    "                self.reward_dict[wall_state] = self.rewards['wall']\n",
    "\n",
    "    def init_maze(self, maze_density: int = None):\n",
    "        \"\"\"\n",
    "        Creates an array of states in a maze environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        maze : np.array\n",
    "            A 2D array of states in a maze environment.\n",
    "        wall_coords : list\n",
    "            A 1D array of wall states in the maze environment.\n",
    "        start_coords: tuple\n",
    "            A tuple of start coordinates in the maze environment.\n",
    "        goal_coords: tuple\n",
    "            A tuple of goal coordinates in the maze environment.\n",
    "        \"\"\"\n",
    "        # If maze_density is None we will create walls in 20% of the maze\n",
    "        if maze_density is None:\n",
    "            maze_density = int(self.nrow_maze * self.ncol_maze * 0.2)\n",
    "\n",
    "        # Creating the wall states\n",
    "        wall_states = self.random_generator.choice(range(1, self.nrow_maze * self.ncol_maze), size=maze_density, replace=False)\n",
    "\n",
    "        # Getting the list of wall coordinates\n",
    "        wall_coords = [self.get_state_coords(s) for s in wall_states]\n",
    "\n",
    "        # Generating a starting state from the states that are NOT walls\n",
    "        start_state = self.random_generator.choice(np.setdiff1d(self.S, wall_states))\n",
    "\n",
    "        # Getting the starting coordinates\n",
    "        start_coords = self.get_state_coords(start_state)\n",
    "\n",
    "        # Generating a goal state from the states that are NOT walls and NOT the starting state\n",
    "        goal_state = self.random_generator.choice(np.setdiff1d(np.setdiff1d(self.S, wall_states), start_state))\n",
    "\n",
    "        # Getting the goal coordinates\n",
    "        goal_coords = self.get_state_coords(goal_state)\n",
    "\n",
    "        # Saving the wall coordinates, start coordinates, and goal coordinates\n",
    "        self.wall_coords = wall_coords\n",
    "        self.start_coords = [start_coords]\n",
    "        self.goal_coords = [goal_coords]\n",
    "\n",
    "        # Saving the indexes for the wall, start and goal states\n",
    "        self.wall_states = wall_states\n",
    "        self.start_state = start_state\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "        # Initiating the reward dictionary \n",
    "        self.init_reward_dict()\n",
    "\n",
    "    def get_state_index(self, row: int, col: int) -> int:\n",
    "        \"\"\"\n",
    "        Returns the state index given the state coordinates. \n",
    "\n",
    "        An inverse function of get_state_coords()\n",
    "        \"\"\"\n",
    "        if (row < 0 or row >= self.nrow_maze or col < 0 or col >= self.ncol_maze):\n",
    "            return -1 \n",
    "        else:\n",
    "            return self.S[row][col]\n",
    "\n",
    "    def get_state_coords(self, s) -> tuple:\n",
    "        \"\"\"\n",
    "        Returns the state coordinates given the state index\n",
    "        \"\"\"\n",
    "        s_index = np.where(self.S == s)\n",
    "        if len(s_index[0]) == 0:\n",
    "            return -1, -1\n",
    "        \n",
    "        return s_index[0][0], s_index[1][0]\n",
    "\n",
    "    def get_action(\n",
    "            self,\n",
    "            ) -> int:\n",
    "        \"\"\"\n",
    "        Returns a random action from the set of actions\n",
    "\n",
    "        The actions are: \n",
    "        0: up\n",
    "        1: down\n",
    "        2: left\n",
    "        3: right\n",
    "        \"\"\"\n",
    "        return self.random_generator.choice(self.actions)\n",
    "    \n",
    "    def argmax(self, q_values: np.array):\n",
    "        \"\"\"argmax with random tie-breaking\n",
    "        Args:\n",
    "            q_values (Numpy array): the array of action values\n",
    "        Returns:\n",
    "            action (int): an action with the highest value\n",
    "        \"\"\"\n",
    "        top = float(\"-inf\")\n",
    "        ties = []\n",
    "\n",
    "        for i in range(len(q_values)):\n",
    "            if q_values[i] > top:\n",
    "                top = q_values[i]\n",
    "                ties = []\n",
    "\n",
    "            if q_values[i] == top:\n",
    "                ties.append(i)\n",
    "\n",
    "        return self.random_generator.choice(ties)\n",
    "\n",
    "    def get_greedy_action(self, state: int) -> int:\n",
    "        \"\"\"\n",
    "        Returns the greedy action given the current state\n",
    "        \"\"\"\n",
    "        # Getting the q values for the current state\n",
    "        q_values = self.Q[state]\n",
    "\n",
    "        # Getting the greedy action\n",
    "        greedy_action = self.argmax(q_values)\n",
    "        \n",
    "        # Returning the greedy action\n",
    "        return greedy_action\n",
    "\n",
    "    def get_epsilon_greedy_action(self, state: int) -> int: \n",
    "        \"\"\"\n",
    "        Returns an epsilon greedy action\n",
    "        \"\"\"\n",
    "        if self.random_generator.random() < self.epsilon:\n",
    "            return self.get_action()\n",
    "        else:\n",
    "            return self.get_greedy_action(state)\n",
    "\n",
    "    # Logging of the agent paths \n",
    "    def log_agent_move(self, state: int):\n",
    "        \"\"\"\n",
    "        Logs the agent's move\n",
    "        \"\"\"\n",
    "        if self.state_visit_paths.get(self.current_episode, None) is None: \n",
    "            self.state_visit_paths[self.current_episode] = [state]\n",
    "        else:\n",
    "            self.state_visit_paths[self.current_episode].append(state)\n",
    "\n",
    "    # Agent learning functions \n",
    "    def update_Q_table(self, new_state: int): \n",
    "        \"\"\"\n",
    "        Function that applies the RL update function\n",
    "        \"\"\" \n",
    "        # Getting the next_state's reward\n",
    "        if new_state != self.past_state:\n",
    "            reward = self.reward_dict[new_state]\n",
    "\n",
    "            # Saving the current Q value\n",
    "            current_Q = self.Q[self.past_state][self.past_action]\n",
    "\n",
    "            # If the new state is the terminal state, then the max_Q is 0\n",
    "            max_Q = 0\n",
    "            \n",
    "            # Else we get the max Q value for the new state\n",
    "            if (new_state != self.goal_state) and (new_state not in self.wall_states):\n",
    "                new_state_Q_values = self.Q[new_state]\n",
    "\n",
    "                # Getting the max Q value \n",
    "                max_Q = np.max(new_state_Q_values)\n",
    "\n",
    "            # Updating inplace the Q value \n",
    "            self.Q[self.past_state][self.past_action] = current_Q + self.alpha * (reward + self.gamma * max_Q - current_Q)\n",
    "    \n",
    "    def get_next_state(self, s: int, action: int) -> int: \n",
    "        \"\"\"\n",
    "        Given the current state and the current action, returns the next state index\n",
    "        \"\"\"\n",
    "        # Getting the state coordinates\n",
    "        s_row, s_col = self.get_state_coords(s)\n",
    "\n",
    "        # Setting the boolean indicating that we have reached the terminal state \n",
    "        reached_terminal = False\n",
    "\n",
    "        # Getting the next state\n",
    "        next_state = -1\n",
    "        if action == 0:\n",
    "            next_state = self.get_state_index(s_row - 1, s_col)\n",
    "        elif action == 1:\n",
    "            next_state = self.get_state_index(s_row + 1, s_col)\n",
    "        elif action == 2:\n",
    "            next_state = self.get_state_index(s_row, s_col - 1)\n",
    "        elif action == 3:\n",
    "            next_state = self.get_state_index(s_row, s_col + 1)\n",
    "        \n",
    "        # If next_state is a wall or the agent is out of bounds, we will stay in the same state\n",
    "        if (next_state == -1) or (next_state in self.wall_states):\n",
    "            return s, reached_terminal\n",
    "\n",
    "        # Incrementing the number of times we have visited the next state\n",
    "        self.increment_state_visit(next_state)\n",
    "\n",
    "        # If next_state is the goal state, we will return to the starting state\n",
    "        if next_state == self.goal_state:\n",
    "            # Incrementing the number of times our agent has reached the goal state\n",
    "            self.num_goal_reached += 1\n",
    "            reached_terminal = True\n",
    "\n",
    "        # Returning the next state\n",
    "        return next_state, reached_terminal\n",
    "\n",
    "    def init_agent(self): \n",
    "        \"\"\"\n",
    "        We will set the past state and past action as the starting state and action\n",
    "        \"\"\"\n",
    "        # Setting the previous state as the starting state\n",
    "        self.past_state = self.start_state\n",
    "        self.past_action = self.get_epsilon_greedy_action(self.past_state)\n",
    "        self.num_goal_reached = 0\n",
    "\n",
    "    def terminal_step(self, new_state: int):\n",
    "        \"\"\"\n",
    "        Updates the agent one last time and resets the agent to the starting position\n",
    "        \"\"\" \n",
    "        # Updating the Q table\n",
    "        self.update_Q_table(new_state)\n",
    "\n",
    "        # Resetting the agent\n",
    "        self.past_state = self.start_state\n",
    "        self.past_action = self.get_epsilon_greedy_action(self.past_state)\n",
    "\n",
    "        # Incrementing the number of times the agent started from the starting state\n",
    "        self.increment_state_visit(self.start_state)\n",
    "\n",
    "        # Incrementing the number of episodes\n",
    "        self.current_episode += 1\n",
    "\n",
    "    def move_agent(self): \n",
    "        \"\"\" \n",
    "        The function that moves the agent to the next state\n",
    "        \"\"\"\n",
    "        # Getting the next state\n",
    "        next_state, reached_terminal = self.get_next_state(self.past_state, self.past_action)\n",
    "\n",
    "        # Adding the next state to the path\n",
    "        self.log_agent_move(next_state)\n",
    "\n",
    "        # Updating the Q table\n",
    "        if not reached_terminal:\n",
    "            self.update_Q_table(next_state)\n",
    "\n",
    "            # Setting the past_state as the next_state\n",
    "            self.past_state = next_state\n",
    "\n",
    "            # Getting the next action\n",
    "            self.past_action = self.get_epsilon_greedy_action(self.past_state)\n",
    "        else: \n",
    "            self.terminal_step(next_state)\n",
    "\n",
    "    def train_episodes(self, num_episodes: int):\n",
    "        \"\"\"\n",
    "        Function that trains the agent for one episode\n",
    "        \"\"\"\n",
    "        # Resetting the agent\n",
    "        self.init_agent()\n",
    "\n",
    "        # Moving the agent until we reach the goal state\n",
    "        while self.current_episode != num_episodes:\n",
    "            self.move_agent()\n",
    "\n",
    "    def create_policy(self): \n",
    "        \"\"\"\n",
    "        Creates a policy dictionary where the key is the state and the value is the action\n",
    "        based on the Q table\n",
    "        \"\"\"\n",
    "        # Creating the policy dictionary\n",
    "        self.policy = {}\n",
    "\n",
    "        # Looping through the states\n",
    "        for state in range(self.S.size):\n",
    "            # Getting the greedy action\n",
    "            greedy_action = self.argmax(self.Q[state])\n",
    "\n",
    "            # Adding the state and action to the policy dictionary\n",
    "            self.policy[state] = greedy_action\n",
    "    \n",
    "    def create_optimal_policy_path(self): \n",
    "        \"\"\"\n",
    "        Creates the path of the optimal policy, starting from the starting state\n",
    "        \"\"\"\n",
    "        # Creating the policy path\n",
    "        self.optimal_policy_path = [self.start_state]\n",
    "        self.optimal_policy_path_coords = [self.get_state_coords(self.start_state)]\n",
    "\n",
    "        # Getting the current state\n",
    "        current_state = self.start_state\n",
    "\n",
    "        # Looping through the states\n",
    "        while current_state != self.goal_state:\n",
    "            # Getting the next state\n",
    "            next_state, _ = self.get_next_state(current_state, self.policy[current_state])\n",
    "\n",
    "            # Adding the next state to the path\n",
    "            self.optimal_policy_path.append(next_state)\n",
    "            self.optimal_policy_path_coords.append(self.get_state_coords(next_state))\n",
    "\n",
    "            # Setting the current state as the next state\n",
    "            current_state = next_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent learning \n",
    "\n",
    "First, let us create the agent object and initiate the maze environment for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an agent object\n",
    "agent = Agent(\n",
    "    nrow_maze=6,\n",
    "    ncol_maze=8,\n",
    "    seed=6,\n",
    "    rewards={'step': 0, 'goal': 10}\n",
    ")\n",
    "\n",
    "# Initiating the maze \n",
    "agent.init_maze(maze_density=11)\n",
    "\n",
    "# Ploting the maze \n",
    "plot_matrix(agent.S, goal_coords=agent.goal_coords, hole_coords=agent.wall_coords, start_coords=agent.start_coords, title=\"Maze\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 25 states, 6 of them are wall states. The agent wants to learn the values for actions in each of the free states that maximizes the total reward. \n",
    "\n",
    "The agent will start exploring from the 23rd state and go to the 10th state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating the agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letting the agent wonder for 1 episode \n",
    "agent.train_episodes(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the policy dictionary \n",
    "agent.create_policy()\n",
    "\n",
    "# Creating the optimal policy path\n",
    "agent.create_optimal_policy_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ploting function for the policy \n",
    "def plot_policy(\n",
    "        S: np.ndarray, \n",
    "        policy: dict, \n",
    "        goal_coords: tuple, \n",
    "        wall_coords: list, \n",
    "        start_coords: tuple, \n",
    "        optimal_policy_path: list,\n",
    "        title: str, \n",
    "        img_width: int = 6, \n",
    "        image_height: int = 6\n",
    "        ):\n",
    "    height, width = S.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    for x in range(height):\n",
    "        for y in range(width):\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the hole_coords list, we color the cell gray \n",
    "            if (x, y) in wall_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell yellow\n",
    "            elif (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='yellow'))\n",
    "\n",
    "            else:\n",
    "                try:\n",
    "                    # If there is a tuple of (x, y) in the start_coords list, we color the cell green\n",
    "                    if (x, y) in start_coords:\n",
    "                        ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='lightgreen'))\n",
    "                    elif (x, y) in optimal_policy_path:\n",
    "                        ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='lightblue'))\n",
    "\n",
    "                    # Adding the arrows to the plot\n",
    "                    if 0 == policy[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n",
    "                    if 1 == policy[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n",
    "                    if 2 == policy[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                    if 3 == policy[S[x, y]]:\n",
    "                        plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    print(f\"Current x and y: {x}, {y}\")\n",
    "                    \n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(\n",
    "    S=agent.S, \n",
    "    policy=agent.policy, \n",
    "    optimal_policy_path=agent.optimal_policy_path_coords,\n",
    "    goal_coords=agent.goal_coords, \n",
    "    wall_coords=agent.wall_coords, \n",
    "    start_coords=agent.start_coords, \n",
    "    title=\"Policy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(agent.S, goal_coords=agent.goal_coords, hole_coords=agent.wall_coords, start_coords=agent.start_coords, title=\"Maze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_state_visits(agent.S, agent.state_visit_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animating the agent exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the first episode's path \n",
    "first_episode_path = agent.state_visit_paths[0]\n",
    "\n",
    "# Converting the state indexes to coords \n",
    "first_episode_path_coords = [agent.get_state_coords(state) for state in first_episode_path]\n",
    "\n",
    "# Defining the path to intermediate images \n",
    "path_to_images = \"q-learning-walking_paths\"\n",
    "\n",
    "# Defining the path to the first episode exploration\n",
    "path_to_first_episode = \"q-learning-walking_paths/episode_1\"\n",
    "\n",
    "# Creating a dir to store the images\n",
    "os.makedirs(path_to_images, exist_ok=True)\n",
    "os.makedirs(path_to_first_episode, exist_ok=True)\n",
    "\n",
    "# If the directory for the first episode is not empty, we delete the files inside it\n",
    "if len(os.listdir(path_to_first_episode)) > 0:\n",
    "    for file in os.listdir(path_to_first_episode):\n",
    "        os.remove(os.path.join(path_to_first_episode, file))\n",
    "\n",
    "# Plotting the first episode's path\n",
    "for i, state in enumerate(first_episode_path_coords):\n",
    "    plot_matrix(\n",
    "        agent.S, \n",
    "        goal_coords=agent.goal_coords, \n",
    "        hole_coords=agent.wall_coords, \n",
    "        start_coords=agent.start_coords, \n",
    "        highlight_coords=[state], \n",
    "        title=f\"Step {i}\",\n",
    "        filename=f\"{path_to_first_episode}/state_{i}.png\"\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the last episode \n",
    "last_episode_path = agent.state_visit_paths[list(agent.state_visit_paths.keys())[-1]]\n",
    "\n",
    "# Converting the state indexes to coords\n",
    "last_episode_path_coords = [agent.get_state_coords(state) for state in last_episode_path]\n",
    "\n",
    "# Defining the path to the last episode exploration\n",
    "path_to_last_episode = f\"{path_to_images}/episode_last\"\n",
    "\n",
    "# Creating a dir to store the images\n",
    "os.makedirs(path_to_last_episode, exist_ok=True)\n",
    "\n",
    "# If the directory for the last episode is not empty, we delete the files inside it\n",
    "if len(os.listdir(path_to_last_episode)) > 0:\n",
    "    for file in os.listdir(path_to_last_episode):\n",
    "        os.remove(os.path.join(path_to_last_episode, file))\n",
    "\n",
    "# Plotting the last episode's path\n",
    "for i, state in enumerate(last_episode_path_coords):\n",
    "    plot_matrix(\n",
    "        agent.S, \n",
    "        goal_coords=agent.goal_coords, \n",
    "        hole_coords=agent.wall_coords, \n",
    "        start_coords=agent.start_coords, \n",
    "        highlight_coords=[state], \n",
    "        title=f\"Step {i}\",\n",
    "        filename=f\"{path_to_last_episode}/state_{i}.png\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a gif from the images\n",
    "from PIL import Image\n",
    "\n",
    "# Creating a directory for GIFs\n",
    "path_to_gifs = \"q-learning-animation\"\n",
    "os.makedirs(path_to_gifs, exist_ok=True)\n",
    "\n",
    "def create_gif(image_dir: str, output_filename: str, duration: int = 300):\n",
    "    images = []\n",
    "    image_files = os.listdir(image_dir)\n",
    "    image_files.sort(key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "    for file in image_files:\n",
    "        images.append(Image.open(os.path.join(image_dir, file)))\n",
    "    images[0].save(\n",
    "        output_filename,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=duration,\n",
    "        loop=0\n",
    "    )\n",
    "\n",
    "# Applying \n",
    "create_gif(path_to_first_episode, f\"{path_to_gifs}/first_episode.gif\")\n",
    "create_gif(path_to_last_episode, f\"{path_to_gifs}/last_episode.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
