{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array math \n",
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning \n",
    "\n",
    "Q learning is an off-policy TD algorithm to approximate the best optimal policy. \n",
    "\n",
    "Definitions: \n",
    "\n",
    "*Q value* - the value given for an action $a$ taken in state $s$. To put it more mathematically, it maps the actions and states to the real number plane: \n",
    "\n",
    "$$Q(a \\in \\mathbb{A}, s \\in \\mathbb{S}) \\rightarrow q \\in  \\mathbb{R}$$\n",
    "\n",
    "In practise, the bigger the q value for an action is, the \"better\" it is for the agent to take. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q table* - a matrix storing the Q values where each row is a state and each column is an action. \n",
    "\n",
    "*$\\epsilon$-greedy policy* - the policy where, at every decision point, the agent takes a random action with the probability of 1 - $\\epsilon$ and the action which has the biggest q value with the probability of $\\epsilon$, where $\\epsilon \\in [0, 1]$.\n",
    "\n",
    "*off-policy algorithm* - algorithms where the agent updates it's policy not using it's behavior policy. \n",
    "\n",
    "*behavior policy* - the policy used by the agent to make an action.\n",
    "\n",
    "Full algorithm for Q learning: \n",
    "\n",
    "1. Hyperparameters: \n",
    "\n",
    "1.1 Define step size $\\alpha \\in (0, 1]$\n",
    "\n",
    "1.2 Define $\\epsilon \\in [0, 1]$\n",
    "\n",
    "1.3 Define the discount factor $\\gamma \\in (0, 1]$\n",
    "\n",
    "1.4 Initialize the Q table where all the values are arbitraty except for terminal states $Q(terminal, *) = 0$\n",
    "\n",
    "1.5 Define the number of episodes $N$. \n",
    "\n",
    "2. Iterate for 1 to $N$:\n",
    "\n",
    "2.1 Pick a starting state s.\n",
    "\n",
    "2.2 Iterate until the agent reaches a terminal state: \n",
    "\n",
    "2.2.1 From the given state, pick an action $A$ using epsilon-greedy policy \n",
    "\n",
    "2.2.2 Take action $A$, observe the transition state $s^{'}$ and the reward $r$\n",
    "\n",
    "**2.2.3 Update the current Q value estimate:** \n",
    "\n",
    "$$ Q(S, A) \\leftarrow Q(S, A) + \\alpha \\left(r + \\gamma \\max_{a} Q(s^{'}, a) - Q(S, A)\\right)$$\n",
    "\n",
    "2.2.4 Set $s$ $\\leftarrow$ $s^{'}$\n",
    "\n",
    "The 2.2.3 update is the main part of the Q algorithm. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in step 2.2.1, the behavior policy is the epslion-greedy policy. \n",
    "\n",
    "But when we update the current Q value, we do not use that behaviour policy - we update the current Q value from the future state Q value and not use the epsilon-greedy policy anywhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "title_model_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d8bd1206f2d7b01971285aecca810bffdaa9045392903017896954e3362baa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
