{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld problem\n",
    "\n",
    "The Gridworld problem in `RL` is a problem where we want to create an optimal strategy for an agent to traverse a grid. A grid is a square matrix of cells, and the agent can move in any of the four directions (up, down, left, right) in each cell. The agent receives a reward of -1 for each step it takes, and a reward of +10 if it reaches the goal cell.\n",
    "\n",
    "In this example, there will be 5 goal cells: one in each corner and the in the center. The agent can start from any non goal squares and has to reach one of the goal cells. The agent can only move in the four directions, and cannot move diagonally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_index_to_matplot_coords(i: int, j: int, n_cols: int) -> Tuple[int, int]:\n",
    "    \"\"\"Converts an array index to a matplot coordinate\"\"\"\n",
    "    x = j\n",
    "    y = n_cols - i - 1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(\n",
    "    M: np.array, \n",
    "    goal_coords: list = [],\n",
    "    img_width: int = 5, \n",
    "    img_height: int = 5, \n",
    "    title: str = None,\n",
    "    annotate_goal: bool = True\n",
    "    ) -> None: \n",
    "    \"\"\"\n",
    "    Plots a matrix as an image.\n",
    "    \"\"\"\n",
    "    height, width = M.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    \n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n",
    "            # so we need to invert the y coordinate to plot the matrix correctly\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "                if annotate_goal:\n",
    "                    ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "            else: \n",
    "                ax.annotate(str(M[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "\n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_policy_matrix(P: dict, S:np.array, goal_coords: list = [], img_width: int = 5, img_height: int = 5, title: str = None) -> None: \n",
    "    \"\"\" \n",
    "    Plots the policy matrix out of the dictionary provided; The dictionary values are used to draw the arrows \n",
    "    \"\"\"\n",
    "    height, width = S.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    ax = fig.add_subplot(111, aspect='equal')\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "            \n",
    "            else:\n",
    "                # Adding the arrows to the plot\n",
    "                if 'up' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'down' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'left' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                if 'right' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "\n",
    "\n",
    "    offset = .5    \n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "def plot_policy_value_matrix(\n",
    "    P: dict, \n",
    "    S: np.array, \n",
    "    V: np.array, \n",
    "    goal_coords: list = [], \n",
    "    img_width: int = 5, \n",
    "    img_height: int = 5, \n",
    "    title: str = None, \n",
    "    annotate_goal: bool = False\n",
    "    ) -> None: \n",
    "    \"\"\" \n",
    "    Plots the policy matrix out of the dictionary provided; The dictionary values are used to draw the arrows \n",
    "    \"\"\"\n",
    "    height, width = S.shape\n",
    "\n",
    "    fig = plt.figure(figsize=(img_width, img_width))\n",
    "    # The first plot is the value matrix \n",
    "    ax = fig.add_subplot(121, aspect='equal')\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            # By default, the (0, 0) coordinate in matplotlib is the bottom left corner,\n",
    "            # so we need to invert the y coordinate to plot the matrix correctly\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "                if annotate_goal:\n",
    "                    ax.annotate(str(V[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "            else: \n",
    "                ax.annotate(str(V[x][y]), xy=(matplot_x, matplot_y), ha='center', va='center')\n",
    "\n",
    "    offset = .5\n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "    ax.set_title('Value Matrix')\n",
    "\n",
    "    # The second plot is the policy matrix \n",
    "    ax = fig.add_subplot(122, aspect='equal')\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            matplot_x, matplot_y = array_index_to_matplot_coords(x, y, height)\n",
    "            \n",
    "            # If there is a tuple of (x, y) in the goal_coords list, we color the cell gray \n",
    "            if (x, y) in goal_coords:\n",
    "                ax.add_patch(matplotlib.patches.Rectangle((matplot_x - 0.5, matplot_y - 0.5), 1, 1, facecolor='gray'))\n",
    "            \n",
    "            else:\n",
    "                # Adding the arrows to the plot\n",
    "                if 'up' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, 0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'down' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0, -0.3, head_width = 0.05, head_length = 0.05)\n",
    "                if 'left' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, -0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "                if 'right' in P[S[x, y]]:\n",
    "                    plt.arrow(matplot_x, matplot_y, 0.3, 0, head_width = 0.05, head_length = 0.05)\n",
    "    \n",
    "    offset = .5\n",
    "    ax.set_xlim(-offset, width - offset)\n",
    "    ax.set_ylim(-offset, height - offset)\n",
    "\n",
    "    ax.hlines(y=np.arange(height+1)- offset, xmin=-offset, xmax=width-offset)\n",
    "    ax.vlines(x=np.arange(width+1) - offset, ymin=-offset, ymax=height-offset)\n",
    "    ax.set_title('Policy Matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices and sets used in an RL problem \n",
    "\n",
    "## Action set\n",
    "\n",
    "The $\\mathbb{A}$ set contains all the possible actions that the agent can take. In this case, the agent can move in any of the four directions, so the action set is $\\mathbb{A} = \\{up, down, left, right\\}$ or $\\mathbb{A} = \\{\\uparrow,  \\downarrow, \\leftarrow, \\rightarrow\\}$\n",
    "\n",
    "##  Reward matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of blocks of a n x n grid \n",
    "n = 7\n",
    "\n",
    "# Defining the value for the hole and the goal\n",
    "goal = 10\n",
    "step = -1\n",
    "\n",
    "# Initiating an empty dataframe of size n x n\n",
    "G = np.ones((n,n))\n",
    "\n",
    "# Defining the coordinates of the goal\n",
    "goal_coords = [(0, n-1), (n-1, 0), (0, 0), (n-1, n-1), (n // 2, n // 2)]\n",
    "#goal_coords = [(1, 2)]\n",
    "# Adding the goal values to the center and the corners\n",
    "for goal_coord in goal_coords:\n",
    "    G[goal_coord[0], goal_coord[1]] = goal\n",
    "\n",
    "# Every other step is -1\n",
    "G[G == 1] = step\n",
    "\n",
    "# Converting the G matrix to int \n",
    "G = G.astype(int)\n",
    "\n",
    "plot_matrix(G, goal_coords, title='Gridworld')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above diagram, the gridworld is represented as a `n` by `n` matrix. Each cell in it represent the reward you get when you transition to that state. We can call the above matrix as the `reward matrix` and denote it $\\mathbb{G}$. Each element in the matrix is a real number: \n",
    "\n",
    "$\\forall r \\in \\mathbb{G}, r \\in \\mathbb{R}$ \n",
    "\n",
    "## State matrix\n",
    "\n",
    "Alongside the $\\mathbb{G}$ matrix, we have the the state matrix $\\mathbb{S}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.arange(0, n*n).reshape(n, n)\n",
    "\n",
    "plot_matrix(S, goal_coords, title='State space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state matrix is just a matrix whose each element gives an index to the grid an agent is in. \n",
    "\n",
    "For simplicity sake, we in the algorithms and the calculations we tend to flatten these matrices and not keep track of the row and the column indices - just the state numbers. We can always go back to the plot above and check were a certain state is. \n",
    "\n",
    "## Policy matrix \n",
    "\n",
    "The policy matrix, denoted as $\\mathbb{P}$ is a matrix whose each element is a probability of taking an action in a certain state. In each of the elements of the grid, the values are an array of all the possible actions an agent can take.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all the unique states to a vector \n",
    "states = np.unique(S)\n",
    "\n",
    "# Dictionary to hold each action for a given state\n",
    "P = {}\n",
    "for s in states: \n",
    "    s_dict = {}\n",
    "\n",
    "    # Checking which index is the current state in the S matrix \n",
    "    s_index = np.where(S == s)\n",
    "\n",
    "    # If the state is in the top left corner, we can only move right and down\n",
    "    if s_index == (0, 0):\n",
    "        s_dict['right'] = 0.5\n",
    "        s_dict['down'] = 0.5\n",
    "    \n",
    "    # If the state is in the top right corner, we can only move left and down\n",
    "    elif s_index == (0, n - 1):\n",
    "        s_dict['left'] = 0.5\n",
    "        s_dict['down'] = 0.5\n",
    "    \n",
    "    # If the state is in the bottom left corner, we can only move right and up\n",
    "    elif s_index == (n - 1, 0):\n",
    "        s_dict['right'] = 0.5\n",
    "        s_dict['up'] = 0.5\n",
    "    \n",
    "    # If the state is in the bottom right corner, we can only move left and up\n",
    "    elif s_index == (n - 1, n - 1):\n",
    "        s_dict['left'] = 0.5\n",
    "        s_dict['up'] = 0.5\n",
    "    \n",
    "    # If the state is in the first row, we can only move left, right, and down\n",
    "    elif s_index[0] == 0:\n",
    "        s_dict['left'] = 0.333\n",
    "        s_dict['right'] = 0.333\n",
    "        s_dict['down'] = 0.333\n",
    "    \n",
    "    # If the state is in the last row, we can only move left, right, and up\n",
    "    elif s_index[0] == n - 1:\n",
    "        s_dict['left'] =  0.333\n",
    "        s_dict['right'] = 0.333\n",
    "        s_dict['up'] = 0.333\n",
    "    \n",
    "    # If the state is in the first column, we can only move up, down, and right\n",
    "    elif s_index[1] == 0:\n",
    "        s_dict['up'] = 0.333\n",
    "        s_dict['down'] = 0.333\n",
    "        s_dict['right'] = 0.333\n",
    "    \n",
    "    # If the state is in the last column, we can only move up, down, and left\n",
    "    elif s_index[1] == n - 1:\n",
    "        s_dict['up'] = 0.333\n",
    "        s_dict['down'] = 0.333\n",
    "        s_dict['left'] = 0.333\n",
    "\n",
    "    # If the state is in the middle, we can move in all directions\n",
    "    else:\n",
    "        s_dict['up'] = 0.25\n",
    "        s_dict['down'] = 0.25\n",
    "        s_dict['left'] = 0.25\n",
    "        s_dict['right'] = 0.25\n",
    "\n",
    "    # Saving the current states trasition probabilities\n",
    "    P[s] = s_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing a plot for the policy matrix with arrows; In one cell there can be the maximum of 4 arrows each indicating the action an agent can take \n",
    "plot_policy_matrix(P, S, goal_coords, title='Policy matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix shows the initial uniform policy - in each state, the probability of transition to any of the four directions is equal to the available actions.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value matrix \n",
    "\n",
    "The value matrix is denoted as $\\mathbb{V}$. The dimension of the matrix is the same as the state space. Each grid in the value matrix represents the total reward an agent can get if it starts from that state and follows the policy. \n",
    "\n",
    "$$v_{\\pi} (s) = \\mathbb{E}\\left[G_{t} | S_{t} = s \\right] $$\n",
    "\n",
    "Where \n",
    "\n",
    "$G_{t}$ - total reward an agent gets after taking action `a` in state `s` and following the policy $\\pi$ till the end of the episode. The equation for $G_{t}$ is given below.\n",
    "\n",
    "$$G_{t} = \\R_{t +1} + \\gamma R_{t + 2} + ... + \\gamma ^{K} R_{t+K}$$\n",
    " \n",
    "$K$ - the number of steps in the episode.\n",
    "\n",
    "$$v_{\\pi} (s) = \\sum_{a \\in \\mathbb{A}} \\left[ \\pi(a | s) \\sum_{s^{'}, r} p(s^{'}, r | s, a) \\left[ r + \\gamma v_{\\pi} (s^{'}) \\right] \\right] $$\n",
    "\n",
    "This equation is called the Bellman equation.\n",
    "\n",
    "Where, \n",
    "\n",
    "$\\pi(a | s)$ is the probability of taking action `a` in state `s`\n",
    "\n",
    "$p(s^{'}, r | s, a)$ is the probability of transitioning to state `s'` with reward `r` when taking action `a` in state `s`\n",
    "\n",
    "$\\gamma \\in (0, 1)$ is the discount factor\n",
    "\n",
    "$v_{\\pi} (s)$ is the value of state `s` under policy $\\pi$. \n",
    "\n",
    "$r$ - reward for taking action `a` in state `s`. \n",
    "\n",
    "The above equation is a recursive one and could go on forever. In practice, we use a finite number of iterations to calculate the value of each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the empty Value function \n",
    "V = np.zeros((n, n))\n",
    "\n",
    "plot_matrix(V, goal_coords, title='Value function', annotate_goal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_value_matrix(P, S, V, goal_coords, title='Policy value matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have not started exploring anything, the value of each state is 0. \n",
    "\n",
    "## Bellman equation simplification for GridWorld \n",
    "\n",
    "After we take an action from any given state, we are guaranteed to get a reward and transition to a new state. Thus, the conditional probability of getting a reward and transitioning to a new state is 1.\n",
    "\n",
    "We can simplify the Bellman equation as follows: \n",
    "\n",
    "$$v_{\\pi} (s) = \\sum_{a \\in \\mathbb{A}} \\left[ \\pi(a | s) \\left[ r + \\gamma v_{\\pi} (s^{'}) \\right] \\right] $$\n",
    "\n",
    "Here\n",
    "\n",
    "$s^{'}$ - the state we transition to after taking action `a` in state `s` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(a: str, s: int, S: np.array): \n",
    "    \"\"\" \n",
    "    Function that returns the next state's coordinates given an action and a state \n",
    "    \"\"\"\n",
    "    # Getting the current indexes \n",
    "    s_index = np.where(S == s)\n",
    "    s_row = s_index[0][0]\n",
    "    s_col = s_index[1][0]\n",
    "\n",
    "    # Defining the indexes of the next state\n",
    "    next_row = s_row \n",
    "    next_col = s_col\n",
    "\n",
    "    if a == 'up':\n",
    "        next_row = s_row - 1\n",
    "        next_col = s_col\n",
    "    elif a == 'down':\n",
    "        next_row = s_row + 1\n",
    "        next_col = s_col\n",
    "    elif a == 'left':\n",
    "        next_row = s_row\n",
    "        next_col = s_col - 1\n",
    "    elif a == 'right':\n",
    "        next_row = s_row\n",
    "        next_col = s_col + 1\n",
    "\n",
    "    return next_row, next_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_value(\n",
    "    s: int, \n",
    "    S: np.array, \n",
    "    P: dict, \n",
    "    G: np.array, \n",
    "    V: np.array, \n",
    "    gamma: float = 0.9\n",
    "    ) -> Tuple: \n",
    "    \"\"\"\n",
    "    Calculates the Belman equation value for the given state\n",
    "    \"\"\"\n",
    "    # Extracting all the available actions for the given state\n",
    "    actions = P[s]\n",
    "\n",
    "    # Placeholder to hold the sum \n",
    "    sum = 0\n",
    "    for action in actions: \n",
    "        # Extracting the probability of the given action \n",
    "        prob = actions[action]\n",
    "\n",
    "        # Getting the next states indexes\n",
    "        next_row, next_col = get_next_state(action, s, S)\n",
    "\n",
    "        # Extracting the expected reward \n",
    "        reward = G[next_row, next_col]\n",
    "\n",
    "        # Extracting the value of the next state\n",
    "        value_prime = V[next_row, next_col]\n",
    "\n",
    "        # Adding to the sum \n",
    "        sum += prob * (reward + gamma * value_prime)\n",
    "\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Conditional probabilities sum of being in 1st state and going right: {bellman_value(1, S, P, G, V)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration for optimal policy \n",
    "\n",
    "We will follow the bellow algorithm to find the best possible policy for the agent to follow.\n",
    "\n",
    "![](media/chapter-4/value-iteration.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration algorithm is defined as: \n",
    "\n",
    "1. Initialize the value matrix $\\mathbb{V}$ with zeros.\n",
    "2. Initialize a constant $\\theta > 0$.\n",
    "3. Loop: \n",
    "    1. Set $\\delta = 0$.\n",
    "    2. For each state `s` in the state space $\\mathbb{S}$:\n",
    "        1. Set $v \\leftarrow V(s)$\n",
    "        2. $V(s) \\leftarrow max_{a} \\left[ r + \\gamma V (s^{'}) \\right]$ \n",
    "        3. Set $\\delta \\leftarrow max(\\delta, |v - V(s)|)$\n",
    "    3. If $\\delta < \\theta$:\n",
    "        Exit the loop.\n",
    "\n",
    "To get the final policy, we can use the following algorithm:\n",
    "\n",
    "$\\pi(s) = \\arg \\max_{a} [r + \\gamma V(s^{'})]$ $\\forall s \\in \\mathbb{S}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_action(\n",
    "    s: int,  \n",
    "    S: np.array, \n",
    "    P: np.array, \n",
    "    G: np.array, \n",
    "    V: np.array, \n",
    "    gamma: float = 0.9\n",
    "    ) -> Tuple:\n",
    "    \"\"\"\n",
    "    Finds the best action given a state\n",
    "\n",
    "    Returns the best action the Bellman value  \n",
    "    \"\"\" \n",
    "    # Extracting all the possible actions for the given state\n",
    "    actions = P[s]\n",
    "\n",
    "    # Initial maximum value \n",
    "    current_max = -np.inf\n",
    "\n",
    "    # The best action is the first action in the dictionary \n",
    "    best_action = list(actions.keys())[0]\n",
    "\n",
    "    # Iterating over the actions \n",
    "    for action in actions: \n",
    "        # Getting the Bellman's value \n",
    "        value = bellman_value(s, S, P, G, V, gamma) \n",
    "\n",
    "        if value > current_max: \n",
    "            current_max = value\n",
    "            best_action = action\n",
    "\n",
    "    return best_action, current_max\n",
    "\n",
    "def update_value(s, S, P, G, V, gamma) -> float:\n",
    "    \"\"\"\n",
    "    Updates the value function for the given state\n",
    "    \"\"\"\n",
    "    # Getting the indexes of s in S \n",
    "    s_index = np.where(S == s)\n",
    "    s_row = s_index[0][0]\n",
    "    s_col = s_index[1][0]\n",
    "\n",
    "    # Getting the best action and the Bellman's value \n",
    "    _, bellman_value = find_best_action(s, S, P, G, V, gamma)\n",
    "\n",
    "    # Rounding up the bellman value\n",
    "    bellman_value = np.round(bellman_value, 2)\n",
    "\n",
    "    # Updating the value function with a rounded value\n",
    "    V[s_row, s_col] = bellman_value\n",
    "\n",
    "    return bellman_value\n",
    "\n",
    "def value_iteration(\n",
    "    S: np.array, \n",
    "    P: np.array, \n",
    "    G: np.array, \n",
    "    V: np.array, \n",
    "    gamma: float = 0.9, \n",
    "    epsilon: float = 0.0001,\n",
    "    n_iter: int = None \n",
    "    ) -> None: \n",
    "    \"\"\"\n",
    "    Function that performs the value iteration algorithm\n",
    "\n",
    "    The function updates the V matrix inplace \n",
    "    \"\"\"\n",
    "    # Iteration tracker \n",
    "    iteration = 0\n",
    "\n",
    "    # Iterating until the difference between the value functions is less than epsilon \n",
    "    iterate = True\n",
    "    while iterate: \n",
    "        # Placeholder for the maximum difference between the value functions \n",
    "        delta = 0\n",
    "        \n",
    "        # Updating the iteration tracker\n",
    "        iteration += 1 \n",
    "        # Iterating over the states \n",
    "        for s in S.flatten():\n",
    "            # Getting the indexes of s in S \n",
    "            s_index = np.where(S == s)\n",
    "            s_row = s_index[0][0]\n",
    "            s_col = s_index[1][0]\n",
    "\n",
    "            # Saving the current value for the state\n",
    "            v_init = V[s_row, s_col].copy()\n",
    "\n",
    "            # Updating the value function\n",
    "            v_new = update_value(s, S, P, G, V, gamma)\n",
    "\n",
    "            # Updating the delta \n",
    "            delta = np.max([delta, np.abs(v_new - v_init)])\n",
    "\n",
    "            if (delta < epsilon) and (n_iter is None): \n",
    "                iterate = False\n",
    "                break\n",
    "\n",
    "        if (n_iter is not None) and (iteration >= n_iter):\n",
    "            iterate = False\n",
    "\n",
    "    # Printing the iteration tracker\n",
    "    print(f\"Converged in {iteration} iterations\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(S, P, V): \n",
    "    \"\"\"\n",
    "    Function that updates the policy given the value function \n",
    "    \"\"\"\n",
    "    # Iterating over the states \n",
    "    for s in S.flatten(): \n",
    "        # Listing all the actions \n",
    "        actions = P[s]\n",
    "\n",
    "        # For each available action, getting the Bellman's value\n",
    "        values = {}\n",
    "        for action in actions.keys():\n",
    "            # Getting the next state indexes\n",
    "            next_row, next_col = get_next_state(action, s, S)\n",
    "\n",
    "            # Saving the value function of that nex t state\n",
    "            values[action] = V[next_row, next_col]\n",
    "        \n",
    "        # Extracting the maximum key value of the values dictionary \n",
    "        max_value = max(values.values())        \n",
    "\n",
    "        # Leaving the keys that are equal to the maximum value\n",
    "        best_actions = [key for key in values if values[key] == max_value]\n",
    "\n",
    "        # Getting the length of the dictionary \n",
    "        length = len(values)\n",
    "\n",
    "        # Creating the final dictionary with all the best actions in it \n",
    "        p_star = {}\n",
    "        for action in best_actions:\n",
    "            p_star[action] = 1/length\n",
    "\n",
    "        # Updating the policy \n",
    "        P[s] = p_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iteration(S, P, G, V, epsilon=10**-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(V, goal_coords, annotate_goal=False, title='Value function')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the policy matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upating the policy \n",
    "update_policy(S, P, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_value_matrix(P, S, V, goal_coords, img_width=11, img_height=11, annotate_goal=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functionalities for reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_policy(S: np.array) -> np.array:\n",
    "    # Saving all the unique states to a vector \n",
    "    states = np.unique(S)\n",
    "\n",
    "    # Saving the shape of the matrix \n",
    "    n = S.shape[0]\n",
    "\n",
    "    # Dictionary to hold each action for a given state\n",
    "    P = {}\n",
    "    for s in states: \n",
    "        s_dict = {}\n",
    "\n",
    "        # Checking which index is the current state in the S matrix \n",
    "        s_index = np.where(S == s)\n",
    "\n",
    "        # If the state is in the top left corner, we can only move right and down\n",
    "        if s_index == (0, 0):\n",
    "            s_dict['right'] = 0.5\n",
    "            s_dict['down'] = 0.5\n",
    "        \n",
    "        # If the state is in the top right corner, we can only move left and down\n",
    "        elif s_index == (0, n - 1):\n",
    "            s_dict['left'] = 0.5\n",
    "            s_dict['down'] = 0.5\n",
    "        \n",
    "        # If the state is in the bottom left corner, we can only move right and up\n",
    "        elif s_index == (n - 1, 0):\n",
    "            s_dict['right'] = 0.5\n",
    "            s_dict['up'] = 0.5\n",
    "        \n",
    "        # If the state is in the bottom right corner, we can only move left and up\n",
    "        elif s_index == (n - 1, n - 1):\n",
    "            s_dict['left'] = 0.5\n",
    "            s_dict['up'] = 0.5\n",
    "        \n",
    "        # If the state is in the first row, we can only move left, right, and down\n",
    "        elif s_index[0] == 0:\n",
    "            s_dict['left'] = 0.333\n",
    "            s_dict['right'] = 0.333\n",
    "            s_dict['down'] = 0.333\n",
    "        \n",
    "        # If the state is in the last row, we can only move left, right, and up\n",
    "        elif s_index[0] == n - 1:\n",
    "            s_dict['left'] =  0.333\n",
    "            s_dict['right'] = 0.333\n",
    "            s_dict['up'] = 0.333\n",
    "        \n",
    "        # If the state is in the first column, we can only move up, down, and right\n",
    "        elif s_index[1] == 0:\n",
    "            s_dict['up'] = 0.333\n",
    "            s_dict['down'] = 0.333\n",
    "            s_dict['right'] = 0.333\n",
    "        \n",
    "        # If the state is in the last column, we can only move up, down, and left\n",
    "        elif s_index[1] == n - 1:\n",
    "            s_dict['up'] = 0.333\n",
    "            s_dict['down'] = 0.333\n",
    "            s_dict['left'] = 0.333\n",
    "\n",
    "        # If the state is in the middle, we can move in all directions\n",
    "        else:\n",
    "            s_dict['up'] = 0.25\n",
    "            s_dict['down'] = 0.25\n",
    "            s_dict['left'] = 0.25\n",
    "            s_dict['right'] = 0.25\n",
    "\n",
    "        # Saving the current states trasition probabilities\n",
    "        P[s] = s_dict\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gridworld(n: int, step_reward: float) -> Tuple: \n",
    "    # Creating the reward matrix \n",
    "    G = np.zeros((n, n)) \n",
    "    G[G == 0] = step_reward\n",
    "\n",
    "    # Initiating the empty value array \n",
    "    V = np.zeros((n, n))\n",
    "\n",
    "    # Creating the state array\n",
    "    S = np.arange(0, n * n).reshape(n, n)\n",
    "\n",
    "    # Initializing the policy\n",
    "    P = init_policy(S)\n",
    "\n",
    "    return S, P, G, V\n",
    "\n",
    "def add_random_goal(G: np.array, goal_reward) -> np.array: \n",
    "    # Extracting the shape of the matrix \n",
    "    n = G.shape[0]\n",
    "    \n",
    "    # Getting random coords\n",
    "    x = np.random.randint(n)\n",
    "    y = np.random.randint(n)\n",
    "\n",
    "    # Adding the goal value inplace\n",
    "    G[x, y] = goal_reward\n",
    "\n",
    "    # Returning the goal coordinates\n",
    "    return x, y\n",
    "\n",
    "def add_goal(G: np.array, goal_reward, x: int, y: int) -> np.array: \n",
    "    # Adding the goal value inplace\n",
    "    G[x, y] = goal_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - random goals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the gridworld\n",
    "S, P, G, V = init_gridworld(n=9, step_reward=-1)\n",
    "\n",
    "print(f\"Shape of the state matrix: {S.shape}\")\n",
    "print(f\"Shape of the policy dictionary: {len(P)}\")\n",
    "print(f\"Shape of the reward matrix: {G.shape}\")\n",
    "print(f\"Shape of the value matrix: {V.shape}\")\n",
    "\n",
    "# Defining the number of random goals to add\n",
    "n_goals = 6\n",
    "\n",
    "# List to store the goal coordinates \n",
    "goal_coords = []\n",
    "for _ in range(n_goals):\n",
    "    # Adding a random goal\n",
    "    goal_coords.append(add_random_goal(G, goal_reward=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(G, goal_coords, annotate_goal=False, title='Reward function', img_height=9, img_width=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_iteration(S, P, G, V, epsilon=10**-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_policy(S, P, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_value_matrix(P, S, V, goal_coords, img_width=13, img_height=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy_matrix(P, S, goal_coords, img_width=10, img_height=10, title='Policy matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2cf5e1b4dfd04b667f9bceb775bba509c4b1aef371dee70d8088b9680fed7c5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
